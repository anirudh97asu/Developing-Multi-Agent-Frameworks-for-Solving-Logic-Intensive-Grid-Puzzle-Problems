{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__run_agentic_framework__:Starting agentic Framework\n",
      "INFO:__run_agentic_framework__:\n",
      "        ╔═══════╗\n",
      "        ║ ◣_◢ ║\n",
      "        ║ {○_○} ║\n",
      "    ╔═══╩═══════╩═══╗\n",
      "    ║  [│_█_█_█_│]  ║\n",
      "    ╚═══╦═══════╦═══╝\n",
      "        ║  ═══  ║\n",
      "        ║   │   ║\n",
      "        ╚═══╦═══╝\n",
      "            ║\n",
      "         ═══╩═══        \n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "from src.llm_engine.gemini_agent import Agent\n",
    "from src.run_agentic_framework import run_multi_agent_system\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>Mistral-7b</th>\n",
       "      <th>Llama-13b</th>\n",
       "      <th>gemini-pro</th>\n",
       "      <th>gpt-4-turbo</th>\n",
       "      <th>Claude-3</th>\n",
       "      <th>actual_question</th>\n",
       "      <th>Clues</th>\n",
       "      <th>categories</th>\n",
       "      <th>number_of_clues</th>\n",
       "      <th>number_categories</th>\n",
       "      <th>category_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>341.json</td>\n",
       "      <td>13</td>\n",
       "      <td>A group of friends has decided to try several ...</td>\n",
       "      <td>3 | Raymond | vegan\\n5 | Wesley | gluten-free\\...</td>\n",
       "      <td>|------------|------------|\\n | 3 | Raymond |\\...</td>\n",
       "      <td>Let's solve the puzzle step by step!\\n \\n Step...</td>\n",
       "      <td>Step-by-step solution:\\n 1. From clue 3, we kn...</td>\n",
       "      <td>Step-by-step solution:\\n \\n 1. From Clue 3, we...</td>\n",
       "      <td>Step-by-step solution:\\n 1. From clue 3, we kn...</td>\n",
       "      <td>A group of friends has decided to try several ...</td>\n",
       "      <td>Clues:\\n1. Stuart lost 4 more pounds than the ...</td>\n",
       "      <td>pounds lost : 3, 5, 7, 9.\\nnames : Raymond, St...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>lost names diets</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        key  id                                           question  \\\n",
       "6  341.json  13  A group of friends has decided to try several ...   \n",
       "\n",
       "                                              answer  \\\n",
       "6  3 | Raymond | vegan\\n5 | Wesley | gluten-free\\...   \n",
       "\n",
       "                                          Mistral-7b  \\\n",
       "6  |------------|------------|\\n | 3 | Raymond |\\...   \n",
       "\n",
       "                                           Llama-13b  \\\n",
       "6  Let's solve the puzzle step by step!\\n \\n Step...   \n",
       "\n",
       "                                          gemini-pro  \\\n",
       "6  Step-by-step solution:\\n 1. From clue 3, we kn...   \n",
       "\n",
       "                                         gpt-4-turbo  \\\n",
       "6  Step-by-step solution:\\n \\n 1. From Clue 3, we...   \n",
       "\n",
       "                                            Claude-3  \\\n",
       "6  Step-by-step solution:\\n 1. From clue 3, we kn...   \n",
       "\n",
       "                                     actual_question  \\\n",
       "6  A group of friends has decided to try several ...   \n",
       "\n",
       "                                               Clues  \\\n",
       "6  Clues:\\n1. Stuart lost 4 more pounds than the ...   \n",
       "\n",
       "                                          categories  number_of_clues  \\\n",
       "6  pounds lost : 3, 5, 7, 9.\\nnames : Raymond, St...                5   \n",
       "\n",
       "   number_categories    category_names  \n",
       "6                  3  lost names diets  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r\"../data/GridPuzzle_processed.csv\")\n",
    "df = df[df[\"id\"].isin([13, 821, 687, 718, 350, 1084, 1677, 1437, 1597, 1935, 1020, 1495, 2893, 2208, 2700, 2788, 3720, 3936, 3642, 3659, 4465, 4658, 4087, 4861, 4140, 4464, 4848, 3954, 5740, 5675, 5212, 4976, 5889, 5659, 5325, 5585, 6843, 6292, 6045, 6432, 6823, 5923, 6778, 6997, 7685, 7865, 7183, 7389, 7866, 7188, 7837, 8712, 7959, 8356, 8142, 7993, 8519, 9106, 9719, 9139, 9487, 8853, 8988, 9043, 9497, 9596, 9965, 10037, 10380, 10100, 10450, 10440, 9976, 10251, 11592, 11097, 11022, 11603, 11546, 11748, 11263, 11367, 11407, 11304, 11321, 11923, 12703, 12356, 12535, 12172, 12133, 12395, 12818, 13003, 12900, 13407, 13198, 12782, 13051, 12980])]\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('A group of friends has decided to try several different weight-loss diets '\n",
      " 'and exercises to see who amongst them can lose the most weight in 3 months. '\n",
      " 'Using only the clues below, match the pounds lost to the options from names '\n",
      " 'and diets. Remember, as with all grid-based logic puzzles, no option in any '\n",
      " 'category will ever be used more than once.\\n'\n",
      " '\\n'\n",
      " 'pounds lost : 3, 5, 7, 9.\\n'\n",
      " 'names : Raymond, Stuart, Tom, Wesley.\\n'\n",
      " 'diets : caveman, dairy-free, gluten-free, vegan.\\n'\n",
      " '\\n'\n",
      " 'Clues:\\n'\n",
      " '1. Stuart lost 4 more pounds than the person who used the vegan diet.\\n'\n",
      " '2. The dieter who used the gluten-free diet lost 2 more pounds than '\n",
      " 'Raymond.\\n'\n",
      " '3. Stuart lost 7 lbs.\\n'\n",
      " '4. The dieter who lost 7 lbs is either the person who used the caveman diet '\n",
      " 'or Raymond.\\n'\n",
      " '5. Wesley lost 5 lbs.\\n'\n",
      " '\\n'\n",
      " 'While answering use the following format:\\n'\n",
      " 'Step-by-step solution:\\n'\n",
      " 'Your steps showing how you are solving the puzzle\\n'\n",
      " 'Final Answer:\\n'\n",
      " 'Fill the following table to show your final answer.\\n'\n",
      " '3 | correct option from names | correct option from diets\\n'\n",
      " '5 | correct option from names | correct option from diets\\n'\n",
      " '7 | correct option from names | correct option from diets\\n'\n",
      " '9 | correct option from names | correct option from diets\\n')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(df[\"question\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(r\"../prompts_info.json\", \"r\") as f:\n",
    "    prompt_json = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 3 48\n",
      "response valid or not:-- 1 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "  1%|          | 1/100 [00:29<48:59, 29.69s/it]INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 3 48\n",
      "response valid or not:-- 1 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "  2%|▏         | 2/100 [00:57<46:43, 28.60s/it]INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 3 48\n",
      "response valid or not:-- 1 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "  3%|▎         | 3/100 [01:27<47:33, 29.41s/it]INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 3 48\n",
      "response valid or not:-- 1 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "  4%|▍         | 4/100 [02:27<1:05:50, 41.15s/it]INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 3 48\n",
      "response valid or not:-- 1 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "  5%|▌         | 5/100 [02:57<58:59, 37.26s/it]  INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 3 48\n",
      "response valid or not:-- 1 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "  6%|▌         | 6/100 [03:27<54:32, 34.81s/it]INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 3 48\n",
      "response valid or not:-- 1 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "  7%|▋         | 7/100 [03:55<50:28, 32.57s/it]INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 3 48\n",
      "response valid or not:-- 1 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "  8%|▊         | 8/100 [04:23<47:50, 31.20s/it]INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 3 48\n",
      "response valid or not:-- 1 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "  9%|▉         | 9/100 [04:56<47:50, 31.55s/it]INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 3 48\n",
      "response valid or not:-- 1 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      " 10%|█         | 10/100 [05:11<39:40, 26.45s/it]INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 3 48\n",
      "response valid or not:-- 1 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      " 11%|█         | 11/100 [05:45<43:00, 28.99s/it]INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 3 48\n",
      "response valid or not:-- 1 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      " 12%|█▏        | 12/100 [06:15<42:48, 29.19s/it]INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 3 48\n",
      "response valid or not:-- 1 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      " 13%|█▎        | 13/100 [06:48<44:00, 30.35s/it]INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 3 48\n",
      "response valid or not:-- 1 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      " 14%|█▍        | 14/100 [07:46<55:38, 38.81s/it]INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 3 48\n",
      "response valid or not:-- 1 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      " 15%|█▌        | 15/100 [08:19<52:27, 37.03s/it]INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 3 48\n",
      "response valid or not:-- 1 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      " 16%|█▌        | 16/100 [08:34<42:30, 30.36s/it]INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 3 48\n",
      "response valid or not:-- 1 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      " 17%|█▋        | 17/100 [09:12<44:56, 32.49s/it]INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 3 48\n",
      "response valid or not:-- 1 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      " 18%|█▊        | 18/100 [09:52<47:38, 34.87s/it]INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75 3 48\n",
      "response valid or not:-- 1 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      " 19%|█▉        | 19/100 [10:35<50:18, 37.26s/it]INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 3 48\n",
      "response valid or not:-- 1 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      " 20%|██        | 20/100 [11:15<50:41, 38.02s/it]INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75 3 48\n",
      "response valid or not:-- 1 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      " 21%|██        | 21/100 [12:26<1:03:24, 48.15s/it]INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 3 48\n",
      "response valid or not:-- 1 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      " 22%|██▏       | 22/100 [13:08<1:00:08, 46.27s/it]INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75 3 48\n",
      "response valid or not:-- 1 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      " 23%|██▎       | 23/100 [14:21<1:09:46, 54.37s/it]INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 3 48\n",
      "response valid or not:-- 1 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      " 24%|██▍       | 24/100 [15:04<1:04:17, 50.75s/it]INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70 3 48\n",
      "response valid or not:-- 1 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      " 25%|██▌       | 25/100 [16:13<1:10:19, 56.26s/it]INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70 3 48\n",
      "response valid or not:-- 1 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      " 26%|██▌       | 26/100 [16:52<48:02, 38.95s/it]  \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "answers = []\n",
    "\n",
    "for index in tqdm(range(df.shape[0])):\n",
    "\n",
    "    if index> 25:\n",
    "        break\n",
    "\n",
    "    try:\n",
    "        final_response = run_multi_agent_system(df=df, index=index, prompt_json=prompt_json, Agent=Agent)\n",
    "        answers.append(final_response)\n",
    "    except Exception as e:\n",
    "        answers.append(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, I've analyzed the probability matrices and the categories. The goal is to match each row number (3, 5, 7, 9) to the correct name and diet based on the highest probability associations.\n",
      "\n",
      "Here's how I interpret the matrices:\n",
      "\n",
      "*   **`pounds lost__x__names`**: This matrix shows the probability of a certain amount of weight loss being associated with a specific name. A value of 1.0 indicates a perfect association.\n",
      "*   **`pounds lost__x__diets`**: This matrix shows the probability of a certain amount of weight loss being associated with a specific diet.\n",
      "*   **`names__x__diets`**: This matrix directly links names to diets.\n",
      "\n",
      "Let's break down the associations:\n",
      "\n",
      "*   **Row 3:**\n",
      "    *   `pounds lost__x__names`: Row 3 has a 1.0 at column 2, meaning it's perfectly associated with \"Stuart\".\n",
      "    *   `pounds lost__x__diets`: Row 3 has a 1.0 at column 1, meaning it's perfectly associated with \"vegan\".\n",
      "    *   `names__x__diets`: Row 2 has a 1.0 at column 1, meaning it's perfectly associated with \"vegan\".\n",
      "*   **Row 5:**\n",
      "    *   `pounds lost__x__names`: Row 1 has a 1.0 at column 0, meaning it's perfectly associated with \"Raymond\".\n",
      "    *   `pounds lost__x__diets`: Row 1 has a 1.0 at column 3, meaning it's perfectly associated with \"dairy-free\".\n",
      "    *   `names__x__diets`: Row 0 has a 1.0 at column 3, meaning it's perfectly associated with \"dairy-free\".\n",
      "*   **Row 7:**\n",
      "    *   `pounds lost__x__names`: Row 2 has a 1.0 at column 1, meaning it's perfectly associated with \"Tom\".\n",
      "    *   `pounds lost__x__diets`: Row 2 has a 1.0 at column 0, meaning it's perfectly associated with \"caveman\".\n",
      "    *   `names__x__diets`: Row 1 has a 1.0 at column 0, meaning it's perfectly associated with \"caveman\".\n",
      "*   **Row 9:**\n",
      "    *   `pounds lost__x__names`: Row 3 has a 1.0 at column 2, meaning it's perfectly associated with \"Wesley\".\n",
      "    *   `pounds lost__x__diets`: Row 3 has a 1.0 at column 2, meaning it's perfectly associated with \"gluten-free\".\n",
      "    *   `names__x__diets`: Row 3 has a 1.0 at column 2, meaning it's perfectly associated with \"gluten-free\".\n",
      "\n",
      "Here's the final table:\n",
      "\n",
      "| Pounds Lost | Names   | Diets        |\n",
      "| :---------- | :------- | :----------- |\n",
      "| 3           | Stuart  | vegan       |\n",
      "| 5           | Raymond | dairy-free  |\n",
      "| 7           | Tom     | caveman      |\n",
      "| 9           | Wesley  | gluten-free |\n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Okay, I will analyze the probability matrices to determine the most likely mapping between players and positions. The goal is to match each player to a specific position based on the highest probability associations.\n",
      "\n",
      "Here's the breakdown of the matrices and how I'll use them:\n",
      "\n",
      "*   **`games__x__players`**: This matrix shows the probability of a game being associated with a particular player.\n",
      "*   **`games__x__positions`**: This matrix shows the probability of a game being associated with a particular position.\n",
      "*   **`players__x__positions`**: This matrix directly links players to positions. The highest probability in each row will indicate the most likely position for that player.\n",
      "\n",
      "Let's analyze `players__x__positions`:\n",
      "\n",
      "```\n",
      "[[0., 1., 0., 0.],\n",
      "[0., 0., 0., 1.],\n",
      "[0., 0., 1., 0.],\n",
      "[1., 0., 0., 0.]]\n",
      "```\n",
      "\n",
      "*   **Row 0:** Highest probability (1.0) is in column 1.\n",
      "*   **Row 1:** Highest probability (1.0) is in column 3.\n",
      "*   **Row 2:** Highest probability (1.0) is in column 2.\n",
      "*   **Row 3:** Highest probability (1.0) is in column 0.\n",
      "\n",
      "Now, let's map these columns to the positions and rows to the players:\n",
      "\n",
      "*   **Players:** Edward Ellis, Gordon Gregg, Jack Jensen, Kurt Koch. (Rows 0, 1, 2, 3 respectively)\n",
      "*   **Positions:** defensive end, placekicker, quarterback, running back. (Columns 0, 1, 2, 3 respectively)\n",
      "\n",
      "Therefore, the mapping is:\n",
      "\n",
      "*   Edward Ellis (Row 0) -> placekicker (Column 1)\n",
      "*   Gordon Gregg (Row 1) -> running back (Column 3)\n",
      "*   Jack Jensen (Row 2) -> quarterback (Column 2)\n",
      "*   Kurt Koch (Row 3) -> defensive end (Column 0)\n",
      "\n",
      "```\n",
      "|       | correct option from players | correct option from positions |\n",
      "| :---- | :-------------------------- | :---------------------------- |\n",
      "| **3** | Kurt Koch                   | defensive end                 |\n",
      "| **4** | Edward Ellis                | placekicker                   |\n",
      "| **5** | Jack Jensen                 | quarterback                  |\n",
      "| **6** | Gordon Gregg                | running back                  |\n",
      "```\n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Okay, I will analyze the probability matrices to determine the correct associations between months, owners, and dog breeds. The matrices represent conditional probabilities. For example, the `months__x__owners` matrix tells us the probability of a specific owner given a specific month.\n",
      "\n",
      "Here's how I'll interpret the matrices:\n",
      "\n",
      "*   `months__x__owners`:  Row `i` represents month `i`, and column `j` represents owner `j`. A value of 1.0 at `[i, j]` means that month `i` is *always* associated with owner `j`.\n",
      "*   `months__x__dog breeds`: Row `i` represents month `i`, and column `j` represents dog breed `j`. A value of 1.0 at `[i, j]` means that month `i` is *always* associated with dog breed `j`.\n",
      "*   `owners__x__dog breeds`: Row `i` represents owner `i`, and column `j` represents dog breed `j`. A value of 1.0 at `[i, j]` means that owner `i` *always* owns dog breed `j`.\n",
      "\n",
      "Given the categories:\n",
      "\n",
      "*   Months: January, February, March, April\n",
      "*   Owners: Kayla, Lydia, Rosie, Violet\n",
      "*   Dog breeds: black lab, boxer, schnauzer, shih tzu\n",
      "\n",
      "Let's map the indices to the categories:\n",
      "\n",
      "*   January = Row 0\n",
      "*   February = Row 1\n",
      "*   March = Row 2\n",
      "*   April = Row 3\n",
      "\n",
      "*   Kayla = Column 0\n",
      "*   Lydia = Column 1\n",
      "*   Rosie = Column 2\n",
      "*   Violet = Column 3\n",
      "\n",
      "*   black lab = Column 0\n",
      "*   boxer = Column 1\n",
      "*   schnauzer = Column 2\n",
      "*   shih tzu = Column 3\n",
      "\n",
      "Now, let's decode the matrices:\n",
      "\n",
      "*   `months__x__owners`:\n",
      "    *   January (Row 0) is associated with Violet (Column 3).\n",
      "    *   February (Row 1) is associated with Rosie (Column 2).\n",
      "    *   March (Row 2) is associated with Lydia (Column 1).\n",
      "    *   April (Row 3) is associated with Kayla (Column 0).\n",
      "\n",
      "*   `months__x__dog breeds`:\n",
      "    *   January (Row 0) is associated with boxer (Column 1).\n",
      "    *   February (Row 1) is associated with shih tzu (Column 3).\n",
      "    *   March (Row 2) is associated with black lab (Column 0).\n",
      "    *   April (Row 3) is associated with schnauzer (Column 2).\n",
      "\n",
      "Now, let's put it all together in the requested table format:\n",
      "\n",
      "| Month     | Owner   | Dog Breed   |\n",
      "|-----------|---------|-------------|\n",
      "| January   | Violet  | boxer       |\n",
      "| February  | Rosie   | shih tzu    |\n",
      "| March     | Lydia   | black lab   |\n",
      "| April     | Kayla   | schnauzer   |\n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Okay, I've analyzed the provided matrices and categories. The matrices are filled with `nan` values, indicating that there's no probabilistic relationship or data available to link the categories directly through the matrices. However, based on the structure and the prompt, I can infer the intended mapping:\n",
      "\n",
      "*   **Months:** January, February, March, April.\n",
      "*   **Probes:** Disty, Iunsel, Moro Jaddo, Viria Mari.\n",
      "*   **Destinations:** Ganymede, Mercury, Pluto, Uranus.\n",
      "\n",
      "Now, I'll create a table to represent the deduced mapping:\n",
      "\n",
      "```\n",
      "| Month     | Correct option from probes | Correct option from destinations |\n",
      "|-----------|-----------------------------|-----------------------------------|\n",
      "| January   | Disty                       | Ganymede                          |\n",
      "| February  | Iunsel                      | Mercury                           |\n",
      "| March     | Moro Jaddo                  | Pluto                             |\n",
      "| April     | Viria Mari                  | Uranus                            |\n",
      "```\n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Okay, I've analyzed the probability matrices and the categories. Here's how I've matched them:\n",
      "\n",
      "*   **`years__x__comets`**: This matrix directly links years to comets with probabilities.\n",
      "*   **`astronomers__x__comets`**: This matrix links astronomers to comets.\n",
      "*   **`years__x__astronomers`**: This matrix has all `NaN` values, indicating no direct probabilistic link was established between years and astronomers in the provided data.\n",
      "\n",
      "Therefore:\n",
      "\n",
      "*   Years: 2008, 2009, 2010, 2011\n",
      "*   Astronomers: Hal Gregory, Jack Ingram, Ken Jones, Underwood\n",
      "*   Comets: Casputi, Crecci, Peinope, Sporrin\n",
      "\n",
      "Now, let's use the `years__x__comets` and `astronomers__x__comets` matrices to deduce the most probable associations:\n",
      "\n",
      "**`years__x__comets` Matrix:**\n",
      "\n",
      "```\n",
      "[[0.        , 0.00496345, 0.        , 1.        ],\n",
      " [1.        , 0.        , 0.        , 0.        ],\n",
      " [0.        , 0.        , 1.        , 0.        ],\n",
      " [0.        , 0.99503655, 0.        , 0.        ]]\n",
      "```\n",
      "\n",
      "This tells us:\n",
      "\n",
      "*   2008 is strongly associated with Sporrin (probability 1.0).\n",
      "*   2009 is strongly associated with Casputi (probability 1.0).\n",
      "*   2010 is strongly associated with Peinope (probability 1.0).\n",
      "*   2011 is strongly associated with Crecci (probability 0.99503655).\n",
      "\n",
      "**`astronomers__x__comets` Matrix:**\n",
      "\n",
      "```\n",
      "[[0., 1., 0., 0.],\n",
      " [0., 0., 0., 1.],\n",
      " [1., 0., 0., 0.],\n",
      " [0., 0., 1., 0.]]\n",
      "```\n",
      "\n",
      "This tells us:\n",
      "\n",
      "*   Hal Gregory is strongly associated with Crecci (probability 1.0).\n",
      "*   Jack Ingram is strongly associated with Sporrin (probability 1.0).\n",
      "*   Ken Jones is strongly associated with Casputi (probability 1.0).\n",
      "*   Underwood is strongly associated with Peinope (probability 1.0).\n",
      "\n",
      "Now, let's put it all together in the requested table format:\n",
      "\n",
      "| Year | Astronomer | Comet |\n",
      "|---|---|---|\n",
      "| 2008 | Jack Ingram | Sporrin |\n",
      "| 2009 | Ken Jones | Casputi |\n",
      "| 2010 | Underwood | Peinope |\n",
      "| 2011 | Hal Gregory | Crecci |\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Let's analyze the given probability matrices to deduce the correct associations between years, superhero names, and real names.\n",
      "\n",
      "*   **`years__x__superheros`**: This matrix shows a one-to-one correspondence between years and superhero names. Specifically:\n",
      "    *   2007 maps to the first superhero.\n",
      "    *   2008 maps to the second superhero.\n",
      "    *   2009 maps to the third superhero.\n",
      "    *   2010 maps to the fourth superhero.\n",
      "\n",
      "*   **`years__x__real names`**: This matrix also shows a one-to-one correspondence between years and real names, but with a slight twist:\n",
      "    *   2007 maps to the second real name.\n",
      "    *   2008 maps to the first real name.\n",
      "    *   2009 maps to the third real name.\n",
      "    *   2010 maps to the fourth real name.\n",
      "\n",
      "*   **`superheros__x__real names`**: This matrix links superhero names to real names:\n",
      "    *   The first superhero maps to the second real name.\n",
      "    *   The second superhero maps to the first real name.\n",
      "    *   The third superhero maps to the fourth real name.\n",
      "    *   The fourth superhero maps to the third real name.\n",
      "\n",
      "Now, let's map the categories to the correct labels:\n",
      "\n",
      "*   Years: `2007, 2008, 2009, 2010.`\n",
      "*   Superheros: `Criminal Bane, Green Avenger, Max Fusion, Ultra Hex.`\n",
      "*   Real Names: `Hal Houston, Lyle Lucas, Peter Powers, Tim Trevino.`\n",
      "\n",
      "Using the matrices, we can now create the table:\n",
      "\n",
      "| Year | Superhero | Real Name |\n",
      "|---|---|---|\n",
      "| 2007 | Criminal Bane | Lyle Lucas |\n",
      "| 2008 | Green Avenger | Hal Houston |\n",
      "| 2009 | Max Fusion | Tim Trevino |\n",
      "| 2010 | Ultra Hex | Peter Powers |\n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Okay, I will analyze the probability matrices to determine the most likely matches between the days, companies, and positions. The matrices represent the probabilities of association between these categories. I'll use the highest probability in each row to make the assignments.\n",
      "\n",
      "*   **days\\_\\_x\\_\\_companies:** This matrix links days to companies.\n",
      "*   **days\\_\\_x\\_\\_positions:** This matrix links days to positions.\n",
      "*   **companies\\_\\_x\\_\\_positions:** This matrix links companies to positions.\n",
      "\n",
      "Let's analyze each day:\n",
      "\n",
      "*   **August 20th:**\n",
      "    *   `days__x__companies`: Highest probability (1.0) is with Sancode.\n",
      "    *   `days__x__positions`: Highest probability (1.0) is with web developer.\n",
      "\n",
      "*   **August 21st:**\n",
      "    *   `days__x__companies`: Highest probability (1.0) is with Haytouch.\n",
      "    *   `days__x__positions`: Highest probability (1.0) is with social media.\n",
      "\n",
      "*   **August 22nd:**\n",
      "    *   `days__x__companies`: Highest probability (1.0) is with Laneplex.\n",
      "    *   `days__x__positions`: Highest probability (1.0) is with copywriter.\n",
      "\n",
      "*   **August 23rd:**\n",
      "    *   `days__x__companies`: Highest probability (1.0) is with Streeter Inc..\n",
      "    *   `days__x__positions`: Highest probability (1.0) is with tech support.\n",
      "\n",
      "Now, let's present the results in a table:\n",
      "\n",
      "|               | correct option from companies | correct option from positions |\n",
      "| :------------ | :---------------------------- | :---------------------------- |\n",
      "| August 20th   | Sancode                       | web developer                 |\n",
      "| August 21st   | Haytouch                      | social media                  |\n",
      "| August 22nd   | Laneplex                      | copywriter                    |\n",
      "| August 23rd   | Streeter Inc.                 | tech support                  |\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Okay, I will analyze the probability matrices to determine the correct matches between days, companies, and positions.\n",
      "\n",
      "The matrix `days__x__companies` tells us the probability of a company being associated with a specific day.  A value of 1.0 indicates a definite association.\n",
      "\n",
      "*   August 20th: Canway (index 0)\n",
      "*   August 21st: Zathdrill (index 3)\n",
      "*   August 22nd: Streeter Inc. (index 2)\n",
      "*   August 23rd: Laneplex (index 1)\n",
      "\n",
      "The matrix `days__x__positions` tells us the probability of a position being associated with a specific day.\n",
      "\n",
      "*   August 20th: sales rep (index 1)\n",
      "*   August 21st: web developer (index 3)\n",
      "*   August 22nd: graphic design (index 0)\n",
      "*   August 23rd: tech support (index 2)\n",
      "\n",
      "Now, let's present the results in a table:\n",
      "\n",
      "| Date        | Company       | Position        |\n",
      "| ----------- | ----------- | ----------- |\n",
      "| August 20th | Canway      | sales rep     |\n",
      "| August 21st | Zathdrill   | web developer |\n",
      "| August 22nd | Streeter Inc.| graphic design|\n",
      "| August 23rd | Laneplex    | tech support  |\n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Okay, I've analyzed the provided probability matrices and categories. The key matrix here is `names__x__careers`, which gives us direct probabilities of association between names and careers. The other two matrices involving birthdays are filled with `nan` values, indicating no probabilistic relationship was established between birthdays and either names or careers.\n",
      "\n",
      "Here's how I'll interpret the `names__x__careers` matrix:\n",
      "\n",
      "*   **Row 1:** `[0., 0., 0., 1.]` means the first name has a probability of 1.0 of being associated with the fourth career.\n",
      "*   **Row 2:** `[0., 0., 1., 0.]` means the second name has a probability of 1.0 of being associated with the third career.\n",
      "*   **Row 3:** `[0., 1., 0., 0.]` means the third name has a probability of 1.0 of being associated with the second career.\n",
      "*   **Row 4:** `[1., 0., 0., 0.]` means the fourth name has a probability of 1.0 of being associated with the first career.\n",
      "\n",
      "Now, let's map these probabilities to the given categories:\n",
      "\n",
      "*   **Names:** Adrian, Dale, Harry, Lamar (in that order)\n",
      "*   **Careers:** architect, lawyer, musician, surgeon (in that order)\n",
      "\n",
      "Therefore, we have the following associations:\n",
      "\n",
      "*   Adrian is a surgeon (probability 1.0)\n",
      "*   Dale is a musician (probability 1.0)\n",
      "*   Harry is a lawyer (probability 1.0)\n",
      "*   Lamar is an architect (probability 1.0)\n",
      "\n",
      "Since the birthday matrices provide no information, we cannot deduce any relationship between birthdays and names or careers. Therefore, the birthdays will simply be listed with their corresponding name and career pairings based on the `names__x__careers` matrix.\n",
      "\n",
      "Here's the final table:\n",
      "\n",
      "| Date       | Name   | Career    |\n",
      "|------------|--------|-----------|\n",
      "| April 13th | Adrian | Surgeon   |\n",
      "| May 18th   | Dale   | Musician  |\n",
      "| June 14th  | Harry  | Lawyer    |\n",
      "| July 16th  | Lamar  | Architect |\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Okay, I've analyzed the probability matrices and will attempt to match the categories based on the highest probabilities within each time slot. The logic is that the highest probability in each row of the `times__x__guides` and `times__x__locations` matrices will correspond to the most likely guide and location, respectively, for that time.\n",
      "\n",
      "Here's the table with the deduced matches:\n",
      "\n",
      "| Time   | Guide (Based on `times__x__guides`) | Location (Based on `times__x__locations`) |\n",
      "| :------- | :----------------------------------- | :--------------------------------------- |\n",
      "| 7:00am | Janice                             | Janice                                  |\n",
      "| 8:00am | Dustin                             | Dustin                                  |\n",
      "| 9:00am | Faye                               | Faye                                     |\n",
      "| 10:00am | Kelly                              | Kelly                                   |\n",
      "\n",
      "**Explanation of the deduction:**\n",
      "\n",
      "*   **7:00am:** The highest probability in the first row of both `times__x__guides` and `times__x__locations` is 0.55194797, which corresponds to the third column. The third item in the guides category is Janice, and the third item in the locations category is High Park.\n",
      "*   **8:00am:** The highest probability in the second row of both `times__x__guides` and `times__x__locations` is 0.54092178, which corresponds to the first column. The first item in the guides category is Dustin, and the first item in the locations category is Casa Loma.\n",
      "*   **9:00am:** The highest probability in the third row of both `times__x__guides` and `times__x__locations` is 0.52829953, which corresponds to the second column. The second item in the guides category is Faye, and the second item in the locations category is Greektown.\n",
      "*   **10:00am:** The highest probability in the fourth row of both `times__x__guides` and `times__x__locations` is 0.51345894, which corresponds to the fourth column. The fourth item in the guides category is Kelly, and the fourth item in the locations category is Little Italy.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Okay, I've analyzed the provided probability matrices and categories. Here's how I'll approach this:\n",
      "\n",
      "1.  **Understanding the Matrices:**\n",
      "\n",
      "    *   `prices__x__companies`: This matrix represents the probabilities (or relationships) between prices and companies.  A value of 1.0 indicates a strong association.  For example, `prices__x__companies[0][3] = 1.0` means the first price is strongly associated with the fourth company.\n",
      "    *   `prices__x__cameras`: This matrix is filled with `NaN` (Not a Number) values. This means there's no direct probabilistic relationship defined between prices and cameras in the given data.  We'll have to infer this relationship through the other matrices.\n",
      "    *   `companies__x__cameras`: This matrix represents the probabilities (or relationships) between companies and cameras. Similar to the first matrix, a value of 1.0 indicates a strong association.\n",
      "\n",
      "2.  **Decoding the Indices:**\n",
      "\n",
      "    *   The order of categories is: `['$550, $575, $600, $625.', 'Banion, Cober, Dayero, Honwa.', 'DM-5000, FC-520, G-290, Tela G5.']`\n",
      "    *   This means:\n",
      "        *   Index 0 corresponds to $550, Index 1 to $575, Index 2 to $600, Index 3 to $625.\n",
      "        *   Index 0 corresponds to Banion, Index 1 to Cober, Index 2 to Dayero, Index 3 to Honwa.\n",
      "        *   Index 0 corresponds to DM-5000, Index 1 to FC-520, Index 2 to G-290, Index 3 to Tela G5.\n",
      "\n",
      "3.  **Mapping Prices to Companies:**\n",
      "\n",
      "    *   From `prices__x__companies`:\n",
      "        *   $550 (Index 0) is associated with Honwa (Index 3).\n",
      "        *   $575 (Index 1) is associated with Cober (Index 1).\n",
      "        *   $600 (Index 2) is associated with Banion (Index 0).\n",
      "        *   $625 (Index 3) is associated with Dayero (Index 2).\n",
      "\n",
      "4.  **Mapping Companies to Cameras:**\n",
      "\n",
      "    *   From `companies__x__cameras`:\n",
      "        *   Banion (Index 0) is associated with FC-520 (Index 1).\n",
      "        *   Cober (Index 1) is associated with DM-5000 (Index 0).\n",
      "        *   Dayero (Index 2) is associated with G-290 (Index 2).\n",
      "        *   Honwa (Index 3) is associated with Tela G5 (Index 3).\n",
      "\n",
      "5.  **Inferring Prices to Cameras:**\n",
      "\n",
      "    *   Now, we combine the two mappings:\n",
      "        *   $550 -> Honwa -> Tela G5\n",
      "        *   $575 -> Cober -> DM-5000\n",
      "        *   $600 -> Banion -> FC-520\n",
      "        *   $625 -> Dayero -> G-290\n",
      "\n",
      "```\n",
      "| Price | Company | Camera |\n",
      "|---|---|---|\n",
      "| $550 | Honwa | Tela G5 |\n",
      "| $575 | Cober | DM-5000 |\n",
      "| $600 | Banion | FC-520 |\n",
      "| $625 | Dayero | G-290 |\n",
      "```\n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Okay, I've analyzed the provided information.  The matrices are all filled with `nan` (Not a Number) values. This means there's no probabilistic relationship or data available to link the categories.  Therefore, I cannot deduce any \"correct\" options.  Any assignment would be arbitrary.\n",
      "\n",
      "Final Answer:\n",
      "\n",
      "Since there is no data to derive the correct options, I will fill the table with \"No Data\" to represent that.\n",
      "\n",
      "| Price | Companies | Cameras |\n",
      "|---|---|---|\n",
      "| $550 | No Data | No Data |\n",
      "| $575 | No Data | No Data |\n",
      "| $600 | No Data | No Data |\n",
      "| $625 | No Data | No Data |\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Okay, let's analyze the provided matrices to deduce the relationships between the rankings (first, second, third, fourth), the teams (Night Ninjas, Oil Crew, Splat Squad, Target Bombs), and the colors (orange, purple, red, yellow).\n",
      "\n",
      "**Understanding the Matrices**\n",
      "\n",
      "*   `rankings__x__teams`: This matrix is filled with `NaN` (Not a Number) values. This indicates that there's no direct probabilistic relationship or information available to link rankings directly to teams. We'll have to infer this relationship indirectly.\n",
      "\n",
      "*   `rankings__x__colors`: This matrix shows the probability of each ranking being associated with each color. The rows represent rankings (first, second, third, fourth), and the columns represent colors (orange, purple, red, yellow).  A value of 1.0 indicates a definite association.\n",
      "\n",
      "*   `teams__x__colors`: This matrix shows the probability of each team being associated with each color. The rows represent teams (Night Ninjas, Oil Crew, Splat Squad, Target Bombs), and the columns represent colors (orange, purple, red, yellow). A value of 1.0 indicates a definite association.\n",
      "\n",
      "**Decoding the Relationships**\n",
      "\n",
      "1.  **`rankings__x__colors` Matrix:**\n",
      "\n",
      "    *   Row 0 (first) has a 1.0 in column 3 (yellow).  This means: **First place is associated with yellow.**\n",
      "    *   Row 1 (second) has a 1.0 in column 2 (red). This means: **Second place is associated with red.**\n",
      "    *   Row 2 (third) has a 1.0 in column 0 (orange). This means: **Third place is associated with orange.**\n",
      "    *   Row 3 (fourth) has a 1.0 in column 1 (purple). This means: **Fourth place is associated with purple.**\n",
      "\n",
      "2.  **`teams__x__colors` Matrix:**\n",
      "\n",
      "    *   Row 0 (Night Ninjas) has a 1.0 in column 2 (red). This means: **Night Ninjas are associated with red.**\n",
      "    *   Row 1 (Oil Crew) has a 1.0 in column 0 (orange). This means: **Oil Crew are associated with orange.**\n",
      "    *   Row 2 (Splat Squad) has a 1.0 in column 3 (yellow). This means: **Splat Squad are associated with yellow.**\n",
      "    *   Row 3 (Target Bombs) has a 1.0 in column 1 (purple). This means: **Target Bombs are associated with purple.**\n",
      "\n",
      "3.  **Combining the Information:**\n",
      "\n",
      "    Now we can link rankings to teams through their shared association with colors:\n",
      "\n",
      "    *   First place (yellow) is associated with Splat Squad.\n",
      "    *   Second place (red) is associated with Night Ninjas.\n",
      "    *   Third place (orange) is associated with Oil Crew.\n",
      "    *   Fourth place (purple) is associated with Target Bombs.\n",
      "\n",
      "**Final Answer:**\n",
      "\n",
      "```\n",
      "| Ranking | Team         | Color  |\n",
      "|---------|--------------|--------|\n",
      "| first   | Splat Squad  | yellow |\n",
      "| second  | Night Ninjas | red    |\n",
      "| third   | Oil Crew     | orange |\n",
      "| fourth  | Target Bombs | purple |\n",
      "```\n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Okay, I've analyzed the provided information.  The matrices all contain `nan` values, indicating that there's no probabilistic relationship established between the categories based on the given data.  Therefore, I cannot deduce any \"correct\" options.  The matrices are essentially empty in terms of providing any meaningful associations.\n",
      "\n",
      "Final Answer:\n",
      "\n",
      "| Price   | Customers | Orders        |\n",
      "|---------|-----------|---------------|\n",
      "| $3.30   | No Data   | No Data       |\n",
      "| $3.40   | No Data   | No Data       |\n",
      "| $3.50   | No Data   | No Data       |\n",
      "| $3.60   | No Data   | No Data       |\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Okay, I will analyze the provided probability matrices to determine the correct associations between distances, teams, and devices. The matrices represent the probabilities of association between the different categories. A value of 1.0 indicates a definite association.\n",
      "\n",
      "*   **distances\\_\\_x\\_\\_teams:** This matrix links distances to teams.\n",
      "*   **distances\\_\\_x\\_\\_devices:** This matrix links distances to devices.\n",
      "*   **teams\\_\\_x\\_\\_devices:** This matrix links teams to devices.\n",
      "\n",
      "Let's denote the categories as follows:\n",
      "\n",
      "*   Distances: 135 ft, 150 ft, 165 ft, 180 ft (indexed 0, 1, 2, 3)\n",
      "*   Teams: Flying Eyes, Russets, Spud Chuckers, Yukon Bolds (indexed 0, 1, 2, 3)\n",
      "*   Devices: catapult, hybrid sling, slingshot, trebuchet (indexed 0, 1, 2, 3)\n",
      "\n",
      "Now, let's analyze the matrices:\n",
      "\n",
      "**1. distances\\_\\_x\\_\\_teams:**\n",
      "\n",
      "*   135 ft (index 0) is associated with Flying Eyes (index 0).\n",
      "*   150 ft (index 1) is associated with Russets (index 1).\n",
      "*   165 ft (index 2) is associated with Spud Chuckers (index 2).\n",
      "*   180 ft (index 3) is associated with Yukon Bolds (index 3).\n",
      "\n",
      "**2. distances\\_\\_x\\_\\_devices:**\n",
      "\n",
      "*   135 ft (index 0) is associated with catapult (index 0).\n",
      "*   150 ft (index 1) is associated with trebuchet (index 3).\n",
      "*   165 ft (index 2) is associated with slingshot (index 2).\n",
      "*   180 ft (index 3) is associated with hybrid sling (index 1).\n",
      "\n",
      "**3. teams\\_\\_x\\_\\_devices:**\n",
      "\n",
      "*   Flying Eyes (index 0) is associated with catapult (index 0).\n",
      "*   Russets (index 1) is associated with hybrid sling (index 1).\n",
      "*   Spud Chuckers (index 2) is associated with trebuchet (index 3).\n",
      "*   Yukon Bolds (index 3) is associated with slingshot (index 2).\n",
      "\n",
      "Now, let's summarize the associations in a table:\n",
      "\n",
      "| Distance | Team          | Device        |\n",
      "| :------- | :------------ | :------------ |\n",
      "| 135 ft   | Flying Eyes   | catapult      |\n",
      "| 150 ft   | Russets       | trebuchet     |\n",
      "| 165 ft   | Spud Chuckers | slingshot     |\n",
      "| 180 ft   | Yukon Bolds   | hybrid sling  |\n",
      "\n",
      "```\n",
      "| Distance | Team          | Device        |\n",
      "| :------- | :------------ | :------------ |\n",
      "| 135 ft   | Flying Eyes   | catapult      |\n",
      "| 150 ft   | Russets       | trebuchet     |\n",
      "| 165 ft   | Spud Chuckers | slingshot     |\n",
      "| 180 ft   | Yukon Bolds   | hybrid sling  |\n",
      "```\n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Let's analyze the given probability matrices and categories to deduce the correct matching.\n",
      "\n",
      "*   **`years__x__shrines`**: This matrix represents the probabilities of transitions between years and shrines.\n",
      "*   **`years__x__towns`**: This matrix represents the probabilities of transitions between years and towns.\n",
      "*   **`shrines__x__towns`**: This matrix represents the probabilities of transitions between shrines and towns.\n",
      "\n",
      "The categories are:\n",
      "\n",
      "1.  **Years**: `1525, 1585, 1645, 1705`\n",
      "2.  **Shrines**: `Amari Michi, Matsuno Okura, Takahashi, Yamasaki`\n",
      "3.  **Towns**: `Nakai, Niwa, Uchida, Usui`\n",
      "\n",
      "Now, let's create the table with the correct options:\n",
      "\n",
      "| Year | Shrine | Town |\n",
      "|---|---|---|\n",
      "| 1525 | Amari Michi | Nakai |\n",
      "| 1585 | Matsuno Okura | Niwa |\n",
      "| 1645 | Takahashi | Uchida |\n",
      "| 1705 | Yamasaki | Usui |\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Based on the provided probability matrices, we can deduce the following mappings:\n",
      "\n",
      "*   **Years:** 1897, 1905, 1913, 1921, 1929\n",
      "*   **Artists:** Ityer, Jenadije, Turxar, Verturu, Zhale\n",
      "*   **Paintings:** Clockwork, Hoxley Hills, Lost in Time, Orange Sky, Willow Bend\n",
      "\n",
      "Now, let's analyze the `years__x__artists` and `years__x__paintings` matrices to determine the most probable artist and painting for each year. We'll select the artist and painting with the highest probability for each year.\n",
      "\n",
      "| Year | Artist      | Painting       |\n",
      "|------|-------------|----------------|\n",
      "| 1897 | Zhale       | Orange Sky     |\n",
      "| 1905 | Turxar      | Willow Bend    |\n",
      "| 1913 | Verturu     | Hoxley Hills   |\n",
      "| 1921 | Jenadije    | Lost in Time   |\n",
      "| 1929 | Ityer       | Clockwork      |\n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Okay, I've analyzed the probability matrices and will now provide the most likely matches between the prices, attendees, and gifts. The matrices represent the conditional probabilities between these categories. I'll look for the highest probabilities in each row to determine the most likely association.\n",
      "\n",
      "Here's how I'll interpret the matrices:\n",
      "\n",
      "*   `prices__x__attendees`:  P(Attendee | Price).  Each row represents a price, and each column represents an attendee. The value in the cell is the probability of that attendee being associated with that price.\n",
      "*   `prices__x__gifts`: P(Gift | Price). Each row represents a price, and each column represents a gift. The value in the cell is the probability of that gift being associated with that price.\n",
      "*   `attendees__x__gifts`: P(Gift | Attendee). Each row represents an attendee and each column represents a gift. The value in the cell is the probability of that gift being associated with that attendee.\n",
      "\n",
      "Now, let's find the most probable associations:\n",
      "\n",
      "*   **$400:**\n",
      "    *   `prices__x__attendees`: Highest probability is with \"Jack Jordan\" (0.9983).\n",
      "    *   `prices__x__gifts`: Highest probability is with \"toaster\" (1.0000).\n",
      "*   **$425:**\n",
      "    *   `prices__x__attendees`: Highest probability is with \"Lily Little\" (1.0000).\n",
      "    *   `prices__x__gifts`: Highest probability is with \"HD tv\" (0.9999).\n",
      "*   **$450:**\n",
      "    *   `prices__x__attendees`: Highest probability is with \"Pam Powell\" (1.0000).\n",
      "    *   `prices__x__gifts`: Highest probability is with \"blu-ray player\" (0.9999).\n",
      "*   **$475:**\n",
      "    *   `prices__x__attendees`: Highest probability is with \"Ned Norris\" (0.9999).\n",
      "    *   `prices__x__gifts`: Highest probability is with \"juicer\" (0.9999).\n",
      "*   **$500:**\n",
      "    *   `prices__x__attendees`: Highest probability is with \"Mitch Mayo\" (1.0000).\n",
      "    *   `prices__x__gifts`: Highest probability is with \"blender\" (0.9992).\n",
      "\n",
      "Here's the table summarizing the results:\n",
      "\n",
      "| Price  | Attendee      | Gift            |\n",
      "| :----- | :------------ | :-------------- |\n",
      "| $400   | Jack Jordan   | toaster         |\n",
      "| $425   | Lily Little   | HD tv           |\n",
      "| $450   | Pam Powell    | blu-ray player  |\n",
      "| $475   | Ned Norris    | juicer          |\n",
      "| $500   | Mitch Mayo    | blender         |\n",
      "\n",
      "```\n",
      "Final Answer:\n",
      "'\n",
      "| Price  | Attendee      | Gift            |\n",
      "| :----- | :------------ | :-------------- |\n",
      "| $400   | Jack Jordan   | toaster         |\n",
      "| $425   | Lily Little   | HD tv           |\n",
      "| $450   | Pam Powell    | blu-ray player  |\n",
      "| $475   | Ned Norris    | juicer          |\n",
      "| $500   | Mitch Mayo    | blender         |\n",
      "'\n",
      "```\n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Let's analyze the probability matrices to determine the most likely associations between the given categories (times, names, and families). The goal is to find the highest probability matches in each matrix.\n",
      "\n",
      "*   **times\\_\\_x\\_\\_names:** This matrix represents the probabilities of association between times and names.\n",
      "    *   Row 1 (12:01am) has a high probability (0.998) with Column 5 (Toni).\n",
      "    *   Row 2 (12:04am) has a probability of 1.0 with Column 3 (Peggy).\n",
      "    *   Row 3 (12:07am) has a probability of 1.0 with Column 4 (Susan).\n",
      "    *   Row 4 (12:10am) has a high probability (0.998) with Column 1 (Iris).\n",
      "    *   Row 5 (12:13am) has a probability of 1.0 with Column 2 (Luis).\n",
      "\n",
      "*   **times\\_\\_x\\_\\_families:** This matrix represents the probabilities of association between times and families.\n",
      "    *   Row 1 (12:01am) has a high probability (0.660) with Column 4 (Quinn).\n",
      "    *   Row 2 (12:04am) has a probability of 1.0 with Column 3 (Ortega).\n",
      "    *   Row 3 (12:07am) has a high probability (0.340) with Column 4 (Quinn).\n",
      "    *   Row 4 (12:10am) has a high probability (0.667) with Column 5 (Vaughan).\n",
      "    *   Row 5 (12:13am) has a probability of 1.0 with Column 1 (Blanchard) and Column 2 (Estrada). Since the probability is the same for both, we will look at the names__x__families matrix to determine which family is more likely to be associated with Luis.\n",
      "\n",
      "*   **names\\_\\_x\\_\\_families:** This matrix represents the probabilities of association between names and families.\n",
      "    *   Row 1 (Iris) has a high probability (0.750) with Column 4 (Quinn).\n",
      "    *   Row 2 (Luis) has a probability of 0.361 with Column 2 (Estrada).\n",
      "    *   Row 3 (Peggy) has a probability of 0.639 with Column 2 (Estrada).\n",
      "    *   Row 4 (Susan) has a high probability (0.500) with Column 5 (Vaughan).\n",
      "    *   Row 5 (Toni) has a probability of 1.0 with Column 1 (Blanchard) and Column 3 (Ortega). Since the probability is the same for both, we will look at the other matrices to determine which family is more likely to be associated with Toni.\n",
      "\n",
      "Based on the analysis, here's the deduced matching:\n",
      "\n",
      "| Time    | Name   | Family    |\n",
      "| :------ | :----- | :-------- |\n",
      "| 12:01am | Toni   | Blanchard |\n",
      "| 12:04am | Peggy  | Ortega    |\n",
      "| 12:07am | Susan  | Vaughan   |\n",
      "| 12:10am | Iris   | Quinn     |\n",
      "| 12:13am | Luis   | Estrada   |\n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Okay, I will analyze the probability matrices to determine the most likely associations between the categories 'days', 'guests', and 'rooms'. The goal is to find the highest probability links between the indices of each matrix row and assign the corresponding category element.\n",
      "\n",
      "Here's the breakdown:\n",
      "\n",
      "**1. `days__x__guests` Matrix:**\n",
      "\n",
      "*   Row 0: Highest probability at index 4 (0.9995).  This links day 2 to guest Yates.\n",
      "*   Row 1: Highest probability at index 3 (0.9999). This links day 3 to guest Owens.\n",
      "*   Row 2: Highest probability at index 1 (0.9999). This links day 4 to guest Hartman.\n",
      "*   Row 3: Highest probability at index 0 (0.9983). This links day 5 to guest Mr. Bray.\n",
      "*   Row 4: Highest probability at index 2 (1.0000). This links day 6 to guest Navarro.\n",
      "\n",
      "**2. `days__x__rooms` Matrix:**\n",
      "\n",
      "*   Row 0: Highest probability at index 0 (0.9999). This links day 2 to room 117.\n",
      "*   Row 1: Highest probability at index 2 (1.0000). This links day 3 to room 203.\n",
      "*   Row 2: Highest probability at index 4 (0.9900). This links day 4 to room 320.\n",
      "*   Row 3: Highest probability at index 3 (0.9899). This links day 5 to room 209.\n",
      "*   Row 4: Highest probability at index 1 (1.0000). This links day 6 to room 124.\n",
      "\n",
      "**3. `guests__x__rooms` Matrix:**\n",
      "\n",
      "*   Row 0: Highest probability at index 1 (0.9999). This links guest Mr. Bray to room 124.\n",
      "*   Row 1: Highest probability at index 4 (0.9983). This links guest Hartman to room 320.\n",
      "*   Row 2: Highest probability at index 3 (0.9998). This links guest Navarro to room 209.\n",
      "*   Row 3: Highest probability at index 0 (0.9981). This links guest Owens to room 117.\n",
      "*   Row 4: Highest probability at index 2 (1.0000). This links guest Yates to room 203.\n",
      "\n",
      "**Final Assignments:**\n",
      "\n",
      "Based on the highest probabilities, here's the deduced table:\n",
      "\n",
      "| Day | Guest      | Room |\n",
      "|-----|------------|------|\n",
      "| 2   | Yates      | 117  |\n",
      "| 3   | Owens      | 203  |\n",
      "| 4   | Hartman    | 320  |\n",
      "| 5   | Mr. Bray   | 209  |\n",
      "| 6   | Navarro    | 124  |\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Okay, I will analyze the probability matrices to determine the most likely associations between the given categories: members (23, 26, 29, 32, 35), fraternities (Alpha Beta Tau, Beta Pi Omega, Lambda Alpha, Sigma Beta, Tau Beta Pi), and streets (Kermit Lane, Park Street, Russell Road, Virginia Street, Wild Avenue).\n",
      "\n",
      "Here's the logic:\n",
      "\n",
      "1.  **`members__x__fraternities`**: This matrix shows the probabilities of members belonging to specific fraternities. High probabilities indicate a strong association.\n",
      "2.  **`members__x__streets`**: This matrix shows the probabilities of members living on specific streets. High probabilities indicate a strong association.\n",
      "3.  **`fraternities__x__streets`**: This matrix shows the probabilities of fraternities being associated with specific streets. High probabilities indicate a strong association.\n",
      "\n",
      "Based on the provided matrices, I'll create the table:\n",
      "\n",
      "```\n",
      "| Member | Fraternity        | Street          |\n",
      "|--------|-------------------|-----------------|\n",
      "| 23     | Tau Beta Pi       | Wild Avenue     |\n",
      "| 26     | Tau Beta Pi       | Wild Avenue     |\n",
      "| 29     | Alpha Beta Tau    | Kermit Lane     |\n",
      "| 32     | Beta Pi Omega     | Park Street     |\n",
      "| 35     | Lambda Alpha      | Russell Road    |\n",
      "```\n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Let's analyze the probability matrices to deduce the relationships between the categories '24, 27, 30, 33, 36.', 'Krista, Nina, Sonia, Tracey, Vicki.', and '4, 7, 10, 12, 15.'.\n",
      "\n",
      "*   **`wins__x__boxers`**: This matrix relates \"wins\" to \"boxers\". High probabilities indicate a strong association.\n",
      "*   **`wins__x__losses`**: This matrix relates \"wins\" to \"losses\". High probabilities indicate a strong association.\n",
      "*   **`boxers__x__losses`**: This matrix directly relates \"boxers\" to \"losses\". High probabilities indicate a strong association.\n",
      "\n",
      "We need to map:\n",
      "\n",
      "*   '24, 27, 30, 33, 36.' to either wins, boxers, or losses.\n",
      "*   'Krista, Nina, Sonia, Tracey, Vicki.' to either wins, boxers, or losses.\n",
      "*   '4, 7, 10, 12, 15.' to either wins, boxers, or losses.\n",
      "\n",
      "By examining the matrices, we can infer the following:\n",
      "\n",
      "*   The `boxers__x__losses` matrix has relatively high probabilities along the diagonal for the first four elements, suggesting a strong one-to-one correspondence between the first four boxers and the first four losses. The last row has a high probability of 1.0 in the fourth column, suggesting a strong association between the last boxer and the fourth loss.\n",
      "*   The `wins__x__boxers` matrix has high probabilities of 1.0 along the diagonal for the first two elements, suggesting a strong one-to-one correspondence between the first two wins and the first two boxers. The last row has a high probability of 1.0 in the first column, suggesting a strong association between the last win and the first boxer.\n",
      "*   The `wins__x__losses` matrix has high probabilities of 0.999999946 for the first element, 0.999999843 for the third element, and 1.0 for the third and fourth elements in the last row, suggesting a strong association between the first win and the first loss, the third win and the second loss, and the last win and the third and fourth losses.\n",
      "\n",
      "Based on these observations, we can deduce the following mapping:\n",
      "\n",
      "*   'Krista, Nina, Sonia, Tracey, Vicki.' corresponds to \"boxers\".\n",
      "*   '4, 7, 10, 12, 15.' corresponds to \"losses\".\n",
      "*   '24, 27, 30, 33, 36.' corresponds to \"wins\".\n",
      "\n",
      "Now, let's create the table:\n",
      "\n",
      "|       | Boxers  | Losses |\n",
      "| :---- | :------- | :------ |\n",
      "| **24** | Krista   | 4      |\n",
      "| **27** | Nina     | 7      |\n",
      "| **30** | Sonia    | 10     |\n",
      "| **33** | Tracey   | 12     |\n",
      "| **36** | Vicki    | 15     |\n",
      "\n",
      "Final Answer:\n",
      "```\n",
      "|       | Boxers  | Losses |\n",
      "| :---- | :------- | :------ |\n",
      "| **24** | Krista   | 4      |\n",
      "| **27** | Nina     | 7      |\n",
      "| **30** | Sonia    | 10     |\n",
      "| **33** | Tracey   | 12     |\n",
      "| **36** | Vicki    | 15     |\n",
      "```\n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "The matrices provided represent conditional probabilities between different categories. Let's analyze each matrix:\n",
      "\n",
      "*   **people\\_\\_x\\_\\_planners:** This matrix shows the probability of a person being associated with a planner. The last row has probabilities of 1.0 for the first four planners and a very small number for the last planner. The first four rows have a probability of 0 for the first four planners and 0.25 for the last planner.\n",
      "\n",
      "*   **people\\_\\_x\\_\\_events:** This matrix shows the probability of a person being associated with an event. The last row has probabilities of 1.0 for the first four events and a very small number for the last event. The first four rows have a probability of 0 for the first four events and 0.25 for the last event.\n",
      "\n",
      "*   **planners\\_\\_x\\_\\_events:** This matrix shows the probability of a planner being associated with an event. The last row has probabilities of 1.0 for the first four events and a very small number for the last event. The first four rows have a probability of 0 for the first four events and 0.25 for the last event.\n",
      "\n",
      "Based on the structure of the matrices, we can infer the following:\n",
      "\n",
      "*   The first category `50, 75, 100, 125, 150` corresponds to the rows of the matrices.\n",
      "*   The second category `Bridget, Flora, Oliver, Patrice, Yolanda` corresponds to \"people\".\n",
      "*   The third category `conference, graduation, retreat, symposium, wedding` corresponds to \"events\".\n",
      "*   The columns of the matrices correspond to \"planners\" and \"events\".\n",
      "\n",
      "The matrices suggest a strong association between the first four \"people\" (Bridget, Flora, Oliver, Patrice) and the last planner/event. The last person (Yolanda) is strongly associated with the first four planners/events.\n",
      "\n",
      "Now, let's create the table as requested:\n",
      "\n",
      "|   | Planners | Events |\n",
      "|---|---|---|\n",
      "| 50  | wedding | wedding |\n",
      "| 75  | wedding | wedding |\n",
      "| 100 | wedding | wedding |\n",
      "| 125 | wedding | wedding |\n",
      "| 150 | Bridget, Flora, Oliver, Patrice | Bridget, Flora, Oliver, Patrice |\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "*   The first four rows (50, 75, 100, 125) have a probability of 0.25 for the last planner/event (wedding) in the `people__x__planners` and `people__x__events` matrices. This indicates a weak association.\n",
      "*   The last row (150) has a probability of 1.0 for the first four planners/events (Bridget, Flora, Oliver, Patrice) in the `people__x__planners` and `people__x__events` matrices. This indicates a strong association.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Let's analyze the probability matrices to determine the correct associations between the categories.\n",
      "\n",
      "*   **votes\\_\\_x\\_\\_contestants:** This matrix represents the probability of a vote belonging to a specific contestant. The rows likely represent different voting patterns or groups, and the columns represent the contestants.\n",
      "*   **votes\\_\\_x\\_\\_sports:** This matrix represents the probability of a vote belonging to a specific sport. The rows likely represent different voting patterns or groups, and the columns represent the sports.\n",
      "*   **contestants\\_\\_x\\_\\_sports:** This matrix represents the probability of a contestant being associated with a specific sport. The rows represent the contestants, and the columns represent the sports.\n",
      "\n",
      "The categories are:\n",
      "\n",
      "*   `['4, 11, 18, 25, 32.', 'Eduardo, Lowell, Marco, Nathaniel, Rudolph.', 'basketball, golf, hockey, lacrosse, soccer.']`\n",
      "\n",
      "Based on the matrix shapes and the category names, we can deduce the following:\n",
      "\n",
      "*   The numbers `4, 11, 18, 25, 32` likely correspond to the rows in the `votes__x__contestants` and `votes__x__sports` matrices. These could represent question IDs or some other grouping of votes.\n",
      "*   `Eduardo, Lowell, Marco, Nathaniel, Rudolph` correspond to the columns of `votes__x__contestants` and the rows of `contestants__x__sports`. These are the contestants.\n",
      "*   `basketball, golf, hockey, lacrosse, soccer` correspond to the columns of `votes__x__sports` and `contestants__x__sports`. These are the sports.\n",
      "\n",
      "Now, let's find the most probable associations for the given indices (4, 11, 18, 25, 32) based on the highest probability values in the matrices. We'll look at `votes__x__contestants` and `votes__x__sports` to determine the most likely contestant and sport for each index.\n",
      "\n",
      "*   **Index 4:**\n",
      "    *   `votes__x__contestants[0]`: Max probability is 0.468 for Marco.\n",
      "    *   `votes__x__sports[0]`: Max probability is 0.999 for Golf.\n",
      "*   **Index 11:**\n",
      "    *   `votes__x__contestants[1]`: Max probability is 0.531 for Marco.\n",
      "    *   `votes__x__sports[1]`: Max probability is 0.999 for Hockey.\n",
      "*   **Index 18:**\n",
      "    *   `votes__x__contestants[2]`: Max probability is 0.666 for Rudolph.\n",
      "    *   `votes__x__sports[2]`: Max probability is 0.998 for Soccer.\n",
      "*   **Index 25:**\n",
      "    *   `votes__x__contestants[3]`: Max probability is 1.000 for Nathaniel.\n",
      "    *   `votes__x__sports[3]`: Max probability is 0.997 for Basketball.\n",
      "*   **Index 32:**\n",
      "    *   `votes__x__contestants[4]`: Max probability is 1.000 for Eduardo.\n",
      "    *   `votes__x__sports[4]`: Max probability is 1.000 for Lacrosse.\n",
      "\n",
      "```\n",
      "| Index | Correct option from contestants | Correct option from sports |\n",
      "|-------|---------------------------------|-----------------------------|\n",
      "| 4     | Marco                           | Golf                        |\n",
      "| 11    | Marco                           | Hockey                      |\n",
      "| 18    | Rudolph                         | Soccer                      |\n",
      "| 25    | Nathaniel                       | Basketball                  |\n",
      "| 32    | Eduardo                         | Lacrosse                    |\n",
      "```\n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Based on the provided probability matrices, we can infer the following mappings:\n",
      "\n",
      "*   **Days:** June 10, June 11, June 12, June 13, June 14. correspond to the rows of the matrices.\n",
      "*   **Names:** Ira, Luke, Nathan, Stanley, Vernon correspond to the columns of the `days__x__names` matrix.\n",
      "*   **Departures:** Buttonwillow, Coulterville, Farley, Hardwick, Kent correspond to the columns of the `days__x__departures` matrix.\n",
      "\n",
      "The matrices `days__x__names` and `days__x__departures` show the probabilities of each name and departure corresponding to each day. The last row (June 14) has probabilities close to 1 for all names and departures, while the other days have a probability of 0.25 only for the last column (Vernon and Kent respectively). This suggests a strong association between June 14 and all names/departures, and a weaker association between the other days and the last name/departure.\n",
      "\n",
      "Here's the table summarizing the findings:\n",
      "\n",
      "|             | correct option from names | correct option from departures |\n",
      "|-------------|---------------------------|--------------------------------|\n",
      "| June 10     | Vernon                    | Kent                             |\n",
      "| June 11     | Vernon                    | Kent                             |\n",
      "| June 12     | Vernon                    | Kent                             |\n",
      "| June 13     | Vernon                    | Kent                             |\n",
      "| June 14     | Ira                       | Buttonwillow                     |\n",
      "| June 14     | Luke                      | Coulterville                     |\n",
      "| June 14     | Nathan                    | Farley                           |\n",
      "| June 14     | Stanley                   | Hardwick                         |\n",
      "| June 14     | Vernon                    | Kent                             |\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Let's analyze the provided probability matrices to deduce the relationships between the categories: prices, winners, and butterflies.\n",
      "\n",
      "*   **prices\\_\\_x\\_\\_winners:** This matrix represents the probabilities of prices given winners. The high probabilities along the diagonal (except for the last row) suggest a strong association between specific prices and specific winners.\n",
      "\n",
      "*   **prices\\_\\_x\\_\\_butterflies:** Similar to the previous matrix, this one shows the probabilities of prices given butterflies. Again, high probabilities along the diagonal (except for the last rows) indicate a strong association between specific prices and specific butterfly types.\n",
      "\n",
      "*   **winners\\_\\_x\\_\\_butterflies:** This matrix represents the probabilities of winners given butterflies. The high probabilities along the diagonal (except for the last rows) suggest a strong association between specific winners and specific butterfly types.\n",
      "\n",
      "Based on these observations, we can infer the following mappings:\n",
      "\n",
      "*   Row 1 corresponds to $45, Gina, and chalkhill.\n",
      "*   Row 2 corresponds to $60, Heather, and grayling.\n",
      "*   Row 3 corresponds to $75, Marlene, and peppered.\n",
      "*   Row 4 corresponds to $90, Phillip, and swallowtail.\n",
      "*   Row 5 corresponds to $105, Victor, and torturix.\n",
      "\n",
      "```\n",
      "| Price | Winner  | Butterfly    |\n",
      "|-------|---------|--------------|\n",
      "| $45   | Gina    | chalkhill    |\n",
      "| $60   | Heather | grayling     |\n",
      "| $75   | Marlene | peppered     |\n",
      "| $90   | Phillip | swallowtail  |\n",
      "| $105  | Victor  | torturix     |\n",
      "```\n",
      "\n",
      "```\n",
      "| Price | Winners | Butterflies |\n",
      "|-------|---------|-------------|\n",
      "| $45   | Gina    | chalkhill   |\n",
      "| $60   | Heather | grayling    |\n",
      "| $75   | Marlene | peppered    |\n",
      "| $90   | Phillip | swallowtail |\n",
      "| $105  | Victor  | torturix    |\n",
      "```\n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for answer in answers:\n",
    "    if answer == \"\":\n",
    "        print(\"yes\")\n",
    "    else:\n",
    "        print(answer)\n",
    "        print(\"--------------------------------------------------------------------------------------------------------------------\")\n",
    "        print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, I've analyzed the probability matrices and the categories. The goal is to match each row number (3, 5, 7, 9) to the correct name and diet based on the highest probability associations.\n",
      "\n",
      "Here's how I interpret the matrices:\n",
      "\n",
      "*   **`pounds lost__x__names`**: This matrix shows the probability of a certain amount of weight loss being associated with a specific name. A value of 1.0 indicates a perfect association.\n",
      "*   **`pounds lost__x__diets`**: This matrix shows the probability of a certain amount of weight loss being associated with a specific diet.\n",
      "*   **`names__x__diets`**: This matrix directly links names to diets.\n",
      "\n",
      "Let's break down the associations:\n",
      "\n",
      "*   **Row 3:**\n",
      "    *   `pounds lost__x__names`: Row 3 has a 1.0 at column 2, meaning it's perfectly associated with \"Stuart\".\n",
      "    *   `pounds lost__x__diets`: Row 3 has a 1.0 at column 1, meaning it's perfectly associated with \"vegan\".\n",
      "    *   `names__x__diets`: Row 2 has a 1.0 at column 1, meaning it's perfectly associated with \"vegan\".\n",
      "*   **Row 5:**\n",
      "    *   `pounds lost__x__names`: Row 1 has a 1.0 at column 0, meaning it's perfectly associated with \"Raymond\".\n",
      "    *   `pounds lost__x__diets`: Row 1 has a 1.0 at column 3, meaning it's perfectly associated with \"dairy-free\".\n",
      "    *   `names__x__diets`: Row 0 has a 1.0 at column 3, meaning it's perfectly associated with \"dairy-free\".\n",
      "*   **Row 7:**\n",
      "    *   `pounds lost__x__names`: Row 2 has a 1.0 at column 1, meaning it's perfectly associated with \"Tom\".\n",
      "    *   `pounds lost__x__diets`: Row 2 has a 1.0 at column 0, meaning it's perfectly associated with \"caveman\".\n",
      "    *   `names__x__diets`: Row 1 has a 1.0 at column 0, meaning it's perfectly associated with \"caveman\".\n",
      "*   **Row 9:**\n",
      "    *   `pounds lost__x__names`: Row 3 has a 1.0 at column 2, meaning it's perfectly associated with \"Wesley\".\n",
      "    *   `pounds lost__x__diets`: Row 3 has a 1.0 at column 2, meaning it's perfectly associated with \"gluten-free\".\n",
      "    *   `names__x__diets`: Row 3 has a 1.0 at column 2, meaning it's perfectly associated with \"gluten-free\".\n",
      "\n",
      "Here's the final table:\n",
      "\n",
      "| Pounds Lost | Names   | Diets        |\n",
      "| :---------- | :------- | :----------- |\n",
      "| 3           | Stuart  | vegan       |\n",
      "| 5           | Raymond | dairy-free  |\n",
      "| 7           | Tom     | caveman      |\n",
      "| 9           | Wesley  | gluten-free |\n"
     ]
    }
   ],
   "source": [
    "print(answers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 | Raymond | vegan\n",
      "5 | Wesley | gluten-free\n",
      "7 | Stuart | caveman\n",
      "9 | Tom | dairy-free\n"
     ]
    }
   ],
   "source": [
    "print(df[\"answer\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def process_answer(string):\n",
    "    lookup_table = {}\n",
    "    for line in string.split(\"\\n\"):\n",
    "        if line != \"\" and re.search(r\"\\w\", line) is not None:\n",
    "            contents = line.split(\"|\")\n",
    "            contents = [val.strip() for val in contents if len(val.strip()) >= 1]\n",
    "        \n",
    "            lookup_table[(contents[0].strip(), contents[1].strip())] = 1\n",
    "            lookup_table[(contents[0].strip(), contents[2].strip())] = 1\n",
    "            lookup_table[(contents[1].strip(), contents[2].strip())] = 1\n",
    "\n",
    "    return lookup_table\n",
    "\n",
    "def get_pred_answer(answer_pred, row_index):\n",
    "    \n",
    "    flag = -1\n",
    "    ckpt = -1\n",
    "\n",
    "    reqd_lines = []\n",
    "\n",
    "    for index, line in enumerate(answer_pred.split(\"\\n\")[::-1]):    \n",
    "\n",
    "        if re.search(r\"---\", line) is not None:\n",
    "            break\n",
    "        \n",
    "        if flag == 1 and ckpt != -1:\n",
    "            break\n",
    "        \n",
    "        pat = re.search(r\"\\|\", line)\n",
    "        if pat is not None:\n",
    "            ckpt = index\n",
    "            reqd_lines.append(line)\n",
    "            continue\n",
    "        \n",
    "        elif pat is None and ckpt != -1:\n",
    "            flag = 1\n",
    "    \n",
    "    op_ = \"\\n\".join(reqd_lines[::-1]) \n",
    "\n",
    "    print(op_)\n",
    "\n",
    "    final_resp = process_answer(op_)\n",
    "\n",
    "    return final_resp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 3           | Stuart  | vegan       |\n",
      "| 5           | Raymond | dairy-free  |\n",
      "| 7           | Tom     | caveman      |\n",
      "| 9           | Wesley  | gluten-free |\n",
      "| **3** | Kurt Koch                   | defensive end                 |\n",
      "| **4** | Edward Ellis                | placekicker                   |\n",
      "| **5** | Jack Jensen                 | quarterback                  |\n",
      "| **6** | Gordon Gregg                | running back                  |\n",
      "| January   | Violet  | boxer       |\n",
      "| February  | Rosie   | shih tzu    |\n",
      "| March     | Lydia   | black lab   |\n",
      "| April     | Kayla   | schnauzer   |\n",
      "| January   | Disty                       | Ganymede                          |\n",
      "| February  | Iunsel                      | Mercury                           |\n",
      "| March     | Moro Jaddo                  | Pluto                             |\n",
      "| April     | Viria Mari                  | Uranus                            |\n",
      "| 2008 | Jack Ingram | Sporrin |\n",
      "| 2009 | Ken Jones | Casputi |\n",
      "| 2010 | Underwood | Peinope |\n",
      "| 2011 | Hal Gregory | Crecci |\n",
      "| 2007 | Criminal Bane | Lyle Lucas |\n",
      "| 2008 | Green Avenger | Hal Houston |\n",
      "| 2009 | Max Fusion | Tim Trevino |\n",
      "| 2010 | Ultra Hex | Peter Powers |\n",
      "| August 20th   | Sancode                       | web developer                 |\n",
      "| August 21st   | Haytouch                      | social media                  |\n",
      "| August 22nd   | Laneplex                      | copywriter                    |\n",
      "| August 23rd   | Streeter Inc.                 | tech support                  |\n",
      "| August 20th | Canway      | sales rep     |\n",
      "| August 21st | Zathdrill   | web developer |\n",
      "| August 22nd | Streeter Inc.| graphic design|\n",
      "| August 23rd | Laneplex    | tech support  |\n",
      "| April 13th | Adrian | Surgeon   |\n",
      "| May 18th   | Dale   | Musician  |\n",
      "| June 14th  | Harry  | Lawyer    |\n",
      "| July 16th  | Lamar  | Architect |\n",
      "| 7:00am | Janice                             | Janice                                  |\n",
      "| 8:00am | Dustin                             | Dustin                                  |\n",
      "| 9:00am | Faye                               | Faye                                     |\n",
      "| 10:00am | Kelly                              | Kelly                                   |\n",
      "| $550 | Honwa | Tela G5 |\n",
      "| $575 | Cober | DM-5000 |\n",
      "| $600 | Banion | FC-520 |\n",
      "| $625 | Dayero | G-290 |\n",
      "| $550 | No Data | No Data |\n",
      "| $575 | No Data | No Data |\n",
      "| $600 | No Data | No Data |\n",
      "| $625 | No Data | No Data |\n",
      "| first   | Splat Squad  | yellow |\n",
      "| second  | Night Ninjas | red    |\n",
      "| third   | Oil Crew     | orange |\n",
      "| fourth  | Target Bombs | purple |\n",
      "| $3.30   | No Data   | No Data       |\n",
      "| $3.40   | No Data   | No Data       |\n",
      "| $3.50   | No Data   | No Data       |\n",
      "| $3.60   | No Data   | No Data       |\n",
      "| 135 ft   | Flying Eyes   | catapult      |\n",
      "| 150 ft   | Russets       | trebuchet     |\n",
      "| 165 ft   | Spud Chuckers | slingshot     |\n",
      "| 180 ft   | Yukon Bolds   | hybrid sling  |\n",
      "| 1525 | Amari Michi | Nakai |\n",
      "| 1585 | Matsuno Okura | Niwa |\n",
      "| 1645 | Takahashi | Uchida |\n",
      "| 1705 | Yamasaki | Usui |\n",
      "| 1897 | Zhale       | Orange Sky     |\n",
      "| 1905 | Turxar      | Willow Bend    |\n",
      "| 1913 | Verturu     | Hoxley Hills   |\n",
      "| 1921 | Jenadije    | Lost in Time   |\n",
      "| 1929 | Ityer       | Clockwork      |\n",
      "| $400   | Jack Jordan   | toaster         |\n",
      "| $425   | Lily Little   | HD tv           |\n",
      "| $450   | Pam Powell    | blu-ray player  |\n",
      "| $475   | Ned Norris    | juicer          |\n",
      "| $500   | Mitch Mayo    | blender         |\n",
      "| 12:01am | Toni   | Blanchard |\n",
      "| 12:04am | Peggy  | Ortega    |\n",
      "| 12:07am | Susan  | Vaughan   |\n",
      "| 12:10am | Iris   | Quinn     |\n",
      "| 12:13am | Luis   | Estrada   |\n",
      "| 2   | Yates      | 117  |\n",
      "| 3   | Owens      | 203  |\n",
      "| 4   | Hartman    | 320  |\n",
      "| 5   | Mr. Bray   | 209  |\n",
      "| 6   | Navarro    | 124  |\n",
      "| 23     | Tau Beta Pi       | Wild Avenue     |\n",
      "| 26     | Tau Beta Pi       | Wild Avenue     |\n",
      "| 29     | Alpha Beta Tau    | Kermit Lane     |\n",
      "| 32     | Beta Pi Omega     | Park Street     |\n",
      "| 35     | Lambda Alpha      | Russell Road    |\n",
      "| **24** | Krista   | 4      |\n",
      "| **27** | Nina     | 7      |\n",
      "| **30** | Sonia    | 10     |\n",
      "| **33** | Tracey   | 12     |\n",
      "| **36** | Vicki    | 15     |\n",
      "| 50  | wedding | wedding |\n",
      "| 75  | wedding | wedding |\n",
      "| 100 | wedding | wedding |\n",
      "| 125 | wedding | wedding |\n",
      "| 150 | Bridget, Flora, Oliver, Patrice | Bridget, Flora, Oliver, Patrice |\n",
      "| 4     | Marco                           | Golf                        |\n",
      "| 11    | Marco                           | Hockey                      |\n",
      "| 18    | Rudolph                         | Soccer                      |\n",
      "| 25    | Nathaniel                       | Basketball                  |\n",
      "| 32    | Eduardo                         | Lacrosse                    |\n",
      "| June 10     | Vernon                    | Kent                             |\n",
      "| June 11     | Vernon                    | Kent                             |\n",
      "| June 12     | Vernon                    | Kent                             |\n",
      "| June 13     | Vernon                    | Kent                             |\n",
      "| June 14     | Ira                       | Buttonwillow                     |\n",
      "| June 14     | Luke                      | Coulterville                     |\n",
      "| June 14     | Nathan                    | Farley                           |\n",
      "| June 14     | Stanley                   | Hardwick                         |\n",
      "| June 14     | Vernon                    | Kent                             |\n",
      "| $45   | Gina    | chalkhill   |\n",
      "| $60   | Heather | grayling    |\n",
      "| $75   | Marlene | peppered    |\n",
      "| $90   | Phillip | swallowtail |\n",
      "| $105  | Victor  | torturix    |\n"
     ]
    }
   ],
   "source": [
    "lookup_table_pred = {}\n",
    "for index,answer in enumerate(answers):\n",
    "    res = get_pred_answer(answer, index)\n",
    "    lookup_table_pred[index] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"Okay, I've analyzed the probability matrices and the categories. The goal is \"\n",
      " 'to match each row number (3, 5, 7, 9) to the correct name and diet based on '\n",
      " 'the highest probability associations.\\n'\n",
      " '\\n'\n",
      " \"Here's how I interpret the matrices:\\n\"\n",
      " '\\n'\n",
      " '*   **`pounds lost__x__names`**: This matrix shows the probability of a '\n",
      " 'certain amount of weight loss being associated with a specific name. A value '\n",
      " 'of 1.0 indicates a perfect association.\\n'\n",
      " '*   **`pounds lost__x__diets`**: This matrix shows the probability of a '\n",
      " 'certain amount of weight loss being associated with a specific diet.\\n'\n",
      " '*   **`names__x__diets`**: This matrix directly links names to diets.\\n'\n",
      " '\\n'\n",
      " \"Let's break down the associations:\\n\"\n",
      " '\\n'\n",
      " '*   **Row 3:**\\n'\n",
      " \"    *   `pounds lost__x__names`: Row 3 has a 1.0 at column 2, meaning it's \"\n",
      " 'perfectly associated with \"Stuart\".\\n'\n",
      " \"    *   `pounds lost__x__diets`: Row 3 has a 1.0 at column 1, meaning it's \"\n",
      " 'perfectly associated with \"vegan\".\\n'\n",
      " \"    *   `names__x__diets`: Row 2 has a 1.0 at column 1, meaning it's \"\n",
      " 'perfectly associated with \"vegan\".\\n'\n",
      " '*   **Row 5:**\\n'\n",
      " \"    *   `pounds lost__x__names`: Row 1 has a 1.0 at column 0, meaning it's \"\n",
      " 'perfectly associated with \"Raymond\".\\n'\n",
      " \"    *   `pounds lost__x__diets`: Row 1 has a 1.0 at column 3, meaning it's \"\n",
      " 'perfectly associated with \"dairy-free\".\\n'\n",
      " \"    *   `names__x__diets`: Row 0 has a 1.0 at column 3, meaning it's \"\n",
      " 'perfectly associated with \"dairy-free\".\\n'\n",
      " '*   **Row 7:**\\n'\n",
      " \"    *   `pounds lost__x__names`: Row 2 has a 1.0 at column 1, meaning it's \"\n",
      " 'perfectly associated with \"Tom\".\\n'\n",
      " \"    *   `pounds lost__x__diets`: Row 2 has a 1.0 at column 0, meaning it's \"\n",
      " 'perfectly associated with \"caveman\".\\n'\n",
      " \"    *   `names__x__diets`: Row 1 has a 1.0 at column 0, meaning it's \"\n",
      " 'perfectly associated with \"caveman\".\\n'\n",
      " '*   **Row 9:**\\n'\n",
      " \"    *   `pounds lost__x__names`: Row 3 has a 1.0 at column 2, meaning it's \"\n",
      " 'perfectly associated with \"Wesley\".\\n'\n",
      " \"    *   `pounds lost__x__diets`: Row 3 has a 1.0 at column 2, meaning it's \"\n",
      " 'perfectly associated with \"gluten-free\".\\n'\n",
      " \"    *   `names__x__diets`: Row 3 has a 1.0 at column 2, meaning it's \"\n",
      " 'perfectly associated with \"gluten-free\".\\n'\n",
      " '\\n'\n",
      " \"Here's the final table:\\n\"\n",
      " '\\n'\n",
      " '| Pounds Lost | Names   | Diets        |\\n'\n",
      " '| :---------- | :------- | :----------- |\\n'\n",
      " '| 3           | Stuart  | vegan       |\\n'\n",
      " '| 5           | Raymond | dairy-free  |\\n'\n",
      " '| 7           | Tom     | caveman      |\\n'\n",
      " '| 9           | Wesley  | gluten-free |')\n"
     ]
    }
   ],
   "source": [
    "pprint(answers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 | Raymond | vegan\n",
      "5 | Wesley | gluten-free\n",
      "7 | Stuart | caveman\n",
      "9 | Tom | dairy-free\n",
      "\n",
      "| :---------- | :------- | :----------- |\n",
      "| 3           | Stuart  | vegan       |\n",
      "| 5           | Raymond | dairy-free  |\n",
      "| 7           | Tom     | caveman      |\n",
      "| 9           | Wesley  | gluten-free |\n",
      "3 | Gordon Gregg | running back\n",
      "4 | Kurt Koch | defensive end\n",
      "5 | Jack Jensen | quarterback\n",
      "6 | Edward Ellis | placekicker\n",
      "\n",
      "| **3** | Kurt Koch                   | defensive end                 |\n",
      "| **4** | Edward Ellis                | placekicker                   |\n",
      "| **5** | Jack Jensen                 | quarterback                  |\n",
      "| **6** | Gordon Gregg                | running back                  |\n",
      "```\n",
      "January | Rosie | shih tzu\n",
      "February | Lydia | black lab\n",
      "March | Kayla | schnauzer\n",
      "April | Violet | boxer\n",
      "\n",
      "|-----------|---------|-------------|\n",
      "| January   | Violet  | boxer       |\n",
      "| February  | Rosie   | shih tzu    |\n",
      "| March     | Lydia   | black lab   |\n",
      "| April     | Kayla   | schnauzer   |\n",
      "January | Viria Mari | Mercury\n",
      "February | Iunsel | Pluto\n",
      "March | Disty | Uranus\n",
      "April | Moro Jaddo | Ganymede\n",
      "\n",
      "| January   | Disty                       | Ganymede                          |\n",
      "| February  | Iunsel                      | Mercury                           |\n",
      "| March     | Moro Jaddo                  | Pluto                             |\n",
      "| April     | Viria Mari                  | Uranus                            |\n",
      "```\n",
      "2008 | Jack Ingram | Peinope\n",
      "2009 | Hal Gregory | Casputi\n",
      "2010 | Underwood | Crecci\n",
      "2011 | Ken Jones | Sporrin\n",
      "\n",
      "| 2008 | Jack Ingram | Sporrin |\n",
      "| 2009 | Ken Jones | Casputi |\n",
      "| 2010 | Underwood | Peinope |\n",
      "| 2011 | Hal Gregory | Crecci |\n",
      "\n",
      "2007 | Criminal Bane | Lyle Lucas\n",
      "2008 | Green Avenger | Hal Houston\n",
      "2009 | Max Fusion | Peter Powers\n",
      "2010 | Ultra Hex | Tim Trevino\n",
      "\n",
      "|---|---|---|\n",
      "| 2007 | Criminal Bane | Lyle Lucas |\n",
      "| 2008 | Green Avenger | Hal Houston |\n",
      "| 2009 | Max Fusion | Tim Trevino |\n",
      "| 2010 | Ultra Hex | Peter Powers |\n",
      "August 20th | Sancode | web developer\n",
      "August 21st | Haytouch | social media\n",
      "August 22nd | Laneplex | tech support\n",
      "August 23rd | Streeter Inc. | copywriter\n",
      "\n",
      "| August 20th   | Sancode                       | web developer                 |\n",
      "| August 21st   | Haytouch                      | social media                  |\n",
      "| August 22nd   | Laneplex                      | copywriter                    |\n",
      "| August 23rd   | Streeter Inc.                 | tech support                  |\n",
      "\n",
      "August 20th | Canway | sales rep\n",
      "August 21st | Zathdrill | web developer\n",
      "August 22nd | Streeter Inc. | graphic design\n",
      "August 23rd | Laneplex | tech support\n",
      "\n",
      "| ----------- | ----------- | ----------- |\n",
      "| August 20th | Canway      | sales rep     |\n",
      "| August 21st | Zathdrill   | web developer |\n",
      "| August 22nd | Streeter Inc.| graphic design|\n",
      "| August 23rd | Laneplex    | tech support  |\n",
      "April 13th | Harry | musician\n",
      "May 18th | Dale | surgeon\n",
      "June 14th | Adrian | lawyer\n",
      "July 16th | Lamar | architect\n",
      "\n",
      "| April 13th | Adrian | Surgeon   |\n",
      "| May 18th   | Dale   | Musician  |\n",
      "| June 14th  | Harry  | Lawyer    |\n",
      "| July 16th  | Lamar  | Architect |\n",
      "\n",
      "7:00am | Faye | High Park\n",
      "8:00am | Kelly | Little Italy\n",
      "9:00am | Janice | Greektown\n",
      "10:00am | Dustin | Casa Loma\n",
      "\n",
      "*   **8:00am:** The highest probability in the second row of both `times__x__guides` and `times__x__locations` is 0.54092178, which corresponds to the first column. The first item in the guides category is Dustin, and the first item in the locations category is Casa Loma.\n",
      "*   **9:00am:** The highest probability in the third row of both `times__x__guides` and `times__x__locations` is 0.52829953, which corresponds to the second column. The second item in the guides category is Faye, and the second item in the locations category is Greektown.\n",
      "*   **10:00am:** The highest probability in the fourth row of both `times__x__guides` and `times__x__locations` is 0.51345894, which corresponds to the fourth column. The fourth item in the guides category is Kelly, and the fourth item in the locations category is Little Italy.\n",
      "\n",
      "\n",
      "$550 | Honwa | G-290\n",
      "$575 | Cober | DM-5000\n",
      "$600 | Banion | Tela G5\n",
      "$625 | Dayero | FC-520\n",
      "\n",
      "| $550 | Honwa | Tela G5 |\n",
      "| $575 | Cober | DM-5000 |\n",
      "| $600 | Banion | FC-520 |\n",
      "| $625 | Dayero | G-290 |\n",
      "```\n",
      "$550 | Torvia | DM-5000\n",
      "$575 | Lirios | Lectra 6\n",
      "$600 | Dayero | Zenix 2C\n",
      "$625 | Nectron | Tela G5\n",
      "\n",
      "| $550 | No Data | No Data |\n",
      "| $575 | No Data | No Data |\n",
      "| $600 | No Data | No Data |\n",
      "| $625 | No Data | No Data |\n",
      "\n",
      "first | Night Ninjas | purple\n",
      "second | Oil Crew | orange\n",
      "third | Target Bombs | red\n",
      "fourth | Splat Squad | yellow\n",
      "\n",
      "| first   | Splat Squad  | yellow |\n",
      "| second  | Night Ninjas | red    |\n",
      "| third   | Oil Crew     | orange |\n",
      "| fourth  | Target Bombs | purple |\n",
      "```\n",
      "$3.30 | Darrell | macchiato\n",
      "$3.40 | Jacob | vanilla latte\n",
      "$3.50 | Charlie | iced americano\n",
      "$3.60 | Henry | espresso\n",
      "\n",
      "| $3.30   | No Data   | No Data       |\n",
      "| $3.40   | No Data   | No Data       |\n",
      "| $3.50   | No Data   | No Data       |\n",
      "| $3.60   | No Data   | No Data       |\n",
      "\n",
      "135 ft | Spud Chuckers | hybrid sling\n",
      "150 ft | Yukon Bolds | catapult\n",
      "165 ft | Flying Eyes | trebuchet\n",
      "180 ft | Russets | slingshot\n",
      "\n",
      "| 135 ft   | Flying Eyes   | catapult      |\n",
      "| 150 ft   | Russets       | trebuchet     |\n",
      "| 165 ft   | Spud Chuckers | slingshot     |\n",
      "| 180 ft   | Yukon Bolds   | hybrid sling  |\n",
      "```\n",
      "1525 | Takahashi | Usui\n",
      "1585 | Yamasaki | Niwa\n",
      "1645 | Amari Michi | Nakai\n",
      "1705 | Matsuno Okura | Uchida\n",
      "\n",
      "| 1585 | Matsuno Okura | Niwa |\n",
      "| 1645 | Takahashi | Uchida |\n",
      "| 1705 | Yamasaki | Usui |\n",
      "\n",
      "\n",
      "1897 | Ityer | Willow Bend\n",
      "1905 | Turxar | Clockwork\n",
      "1913 | Verturu | Lost in Time\n",
      "1921 | Zhale | Hoxley Hills\n",
      "1929 | Jenadije | Orange Sky\n",
      "\n",
      "| 1897 | Zhale       | Orange Sky     |\n",
      "| 1905 | Turxar      | Willow Bend    |\n",
      "| 1913 | Verturu     | Hoxley Hills   |\n",
      "| 1921 | Jenadije    | Lost in Time   |\n",
      "| 1929 | Ityer       | Clockwork      |\n",
      "$400 | Lily Little | HD tv\n",
      "$425 | Jack Jordan | toaster\n",
      "$450 | Ned Norris | juicer\n",
      "$475 | Pam Powell | blu-ray player\n",
      "$500 | Mitch Mayo | blender\n",
      "\n",
      "| $450   | Pam Powell    | blu-ray player  |\n",
      "| $475   | Ned Norris    | juicer          |\n",
      "| $500   | Mitch Mayo    | blender         |\n",
      "'\n",
      "```\n",
      "12:01am | Iris | Vaughan\n",
      "12:04am | Peggy | Ortega\n",
      "12:07am | Luis | Estrada\n",
      "12:10am | Susan | Blanchard\n",
      "12:13am | Toni | Quinn\n",
      "\n",
      "| 12:01am | Toni   | Blanchard |\n",
      "| 12:04am | Peggy  | Ortega    |\n",
      "| 12:07am | Susan  | Vaughan   |\n",
      "| 12:10am | Iris   | Quinn     |\n",
      "| 12:13am | Luis   | Estrada   |\n",
      "2 | Owens | 117\n",
      "3 | Yates | 203\n",
      "4 | Hartman | 209\n",
      "5 | Navarro | 320\n",
      "6 | Mr. Bray | 124\n",
      "\n",
      "| 3   | Owens      | 203  |\n",
      "| 4   | Hartman    | 320  |\n",
      "| 5   | Mr. Bray   | 209  |\n",
      "| 6   | Navarro    | 124  |\n",
      "\n",
      "23 | Sigma Beta | Russell Road\n",
      "26 | Beta Pi Omega | Park Street\n",
      "29 | Alpha Beta Tau | Virginia Street\n",
      "32 | Tau Beta Pi | Wild Avenue\n",
      "35 | Lambda Alpha | Kermit Lane\n",
      "\n",
      "| 26     | Tau Beta Pi       | Wild Avenue     |\n",
      "| 29     | Alpha Beta Tau    | Kermit Lane     |\n",
      "| 32     | Beta Pi Omega     | Park Street     |\n",
      "| 35     | Lambda Alpha      | Russell Road    |\n",
      "```\n",
      "24 | Vicki | 4\n",
      "27 | Krista | 7\n",
      "30 | Tracey | 15\n",
      "33 | Nina | 12\n",
      "36 | Sonia | 10\n",
      "\n",
      "| **27** | Nina     | 7      |\n",
      "| **30** | Sonia    | 10     |\n",
      "| **33** | Tracey   | 12     |\n",
      "| **36** | Vicki    | 15     |\n",
      "```\n",
      "50 | Yolanda | symposium\n",
      "75 | Flora | wedding\n",
      "100 | Bridget | conference\n",
      "125 | Patrice | retreat\n",
      "150 | Oliver | graduation\n",
      "\n",
      "\n",
      "*   The first four rows (50, 75, 100, 125) have a probability of 0.25 for the last planner/event (wedding) in the `people__x__planners` and `people__x__events` matrices. This indicates a weak association.\n",
      "*   The last row (150) has a probability of 1.0 for the first four planners/events (Bridget, Flora, Oliver, Patrice) in the `people__x__planners` and `people__x__events` matrices. This indicates a strong association.\n",
      "\n",
      "\n",
      "4 | Lowell | golf\n",
      "11 | Marco | soccer\n",
      "18 | Nathaniel | hockey\n",
      "25 | Eduardo | basketball\n",
      "32 | Rudolph | lacrosse\n",
      "\n",
      "| 11    | Marco                           | Hockey                      |\n",
      "| 18    | Rudolph                         | Soccer                      |\n",
      "| 25    | Nathaniel                       | Basketball                  |\n",
      "| 32    | Eduardo                         | Lacrosse                    |\n",
      "```\n",
      "June 10 | Stanley | Kent\n",
      "June 11 | Luke | Farley\n",
      "June 12 | Vernon | Coulterville\n",
      "June 13 | Nathan | Buttonwillow\n",
      "June 14 | Ira | Hardwick\n",
      "\n",
      "| June 14     | Luke                      | Coulterville                     |\n",
      "| June 14     | Nathan                    | Farley                           |\n",
      "| June 14     | Stanley                   | Hardwick                         |\n",
      "| June 14     | Vernon                    | Kent                             |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "correct_hm = []\n",
    "correct_sf = []\n",
    "for indx in range(25):\n",
    "    print(df[\"answer\"].iloc[indx])\n",
    "    print()\n",
    "    print(\"\\n\".join(answers[indx].split(\"\\n\")[-5:]))\n",
    "\n",
    "    a = input()\n",
    "    b = input()\n",
    "\n",
    "    if a == \"1\":\n",
    "        correct_hm.append(1)\n",
    "\n",
    "    \n",
    "    if b == \"1\":\n",
    "        correct_sf.append(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(correct_hm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08\n",
      "0.32\n"
     ]
    }
   ],
   "source": [
    "print(sum(correct_hm)/ 25)\n",
    "print(sum(correct_sf) / 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {('3', 'Stuart'): 1,\n",
       "  ('3', 'vegan'): 1,\n",
       "  ('Stuart', 'vegan'): 1,\n",
       "  ('5', 'Raymond'): 1,\n",
       "  ('5', 'dairy-free'): 1,\n",
       "  ('Raymond', 'dairy-free'): 1,\n",
       "  ('7', 'Tom'): 1,\n",
       "  ('7', 'caveman'): 1,\n",
       "  ('Tom', 'caveman'): 1,\n",
       "  ('9', 'Wesley'): 1,\n",
       "  ('9', 'gluten-free'): 1,\n",
       "  ('Wesley', 'gluten-free'): 1},\n",
       " 1: {('**3**', 'Kurt Koch'): 1,\n",
       "  ('**3**', 'defensive end'): 1,\n",
       "  ('Kurt Koch', 'defensive end'): 1,\n",
       "  ('**4**', 'Edward Ellis'): 1,\n",
       "  ('**4**', 'placekicker'): 1,\n",
       "  ('Edward Ellis', 'placekicker'): 1,\n",
       "  ('**5**', 'Jack Jensen'): 1,\n",
       "  ('**5**', 'quarterback'): 1,\n",
       "  ('Jack Jensen', 'quarterback'): 1,\n",
       "  ('**6**', 'Gordon Gregg'): 1,\n",
       "  ('**6**', 'running back'): 1,\n",
       "  ('Gordon Gregg', 'running back'): 1},\n",
       " 2: {('January', 'Violet'): 1,\n",
       "  ('January', 'boxer'): 1,\n",
       "  ('Violet', 'boxer'): 1,\n",
       "  ('February', 'Rosie'): 1,\n",
       "  ('February', 'shih tzu'): 1,\n",
       "  ('Rosie', 'shih tzu'): 1,\n",
       "  ('March', 'Lydia'): 1,\n",
       "  ('March', 'black lab'): 1,\n",
       "  ('Lydia', 'black lab'): 1,\n",
       "  ('April', 'Kayla'): 1,\n",
       "  ('April', 'schnauzer'): 1,\n",
       "  ('Kayla', 'schnauzer'): 1},\n",
       " 3: {('January', 'Disty'): 1,\n",
       "  ('January', 'Ganymede'): 1,\n",
       "  ('Disty', 'Ganymede'): 1,\n",
       "  ('February', 'Iunsel'): 1,\n",
       "  ('February', 'Mercury'): 1,\n",
       "  ('Iunsel', 'Mercury'): 1,\n",
       "  ('March', 'Moro Jaddo'): 1,\n",
       "  ('March', 'Pluto'): 1,\n",
       "  ('Moro Jaddo', 'Pluto'): 1,\n",
       "  ('April', 'Viria Mari'): 1,\n",
       "  ('April', 'Uranus'): 1,\n",
       "  ('Viria Mari', 'Uranus'): 1},\n",
       " 4: {('2008', 'Jack Ingram'): 1,\n",
       "  ('2008', 'Sporrin'): 1,\n",
       "  ('Jack Ingram', 'Sporrin'): 1,\n",
       "  ('2009', 'Ken Jones'): 1,\n",
       "  ('2009', 'Casputi'): 1,\n",
       "  ('Ken Jones', 'Casputi'): 1,\n",
       "  ('2010', 'Underwood'): 1,\n",
       "  ('2010', 'Peinope'): 1,\n",
       "  ('Underwood', 'Peinope'): 1,\n",
       "  ('2011', 'Hal Gregory'): 1,\n",
       "  ('2011', 'Crecci'): 1,\n",
       "  ('Hal Gregory', 'Crecci'): 1},\n",
       " 5: {('2007', 'Criminal Bane'): 1,\n",
       "  ('2007', 'Lyle Lucas'): 1,\n",
       "  ('Criminal Bane', 'Lyle Lucas'): 1,\n",
       "  ('2008', 'Green Avenger'): 1,\n",
       "  ('2008', 'Hal Houston'): 1,\n",
       "  ('Green Avenger', 'Hal Houston'): 1,\n",
       "  ('2009', 'Max Fusion'): 1,\n",
       "  ('2009', 'Tim Trevino'): 1,\n",
       "  ('Max Fusion', 'Tim Trevino'): 1,\n",
       "  ('2010', 'Ultra Hex'): 1,\n",
       "  ('2010', 'Peter Powers'): 1,\n",
       "  ('Ultra Hex', 'Peter Powers'): 1},\n",
       " 6: {('August 20th', 'Sancode'): 1,\n",
       "  ('August 20th', 'web developer'): 1,\n",
       "  ('Sancode', 'web developer'): 1,\n",
       "  ('August 21st', 'Haytouch'): 1,\n",
       "  ('August 21st', 'social media'): 1,\n",
       "  ('Haytouch', 'social media'): 1,\n",
       "  ('August 22nd', 'Laneplex'): 1,\n",
       "  ('August 22nd', 'copywriter'): 1,\n",
       "  ('Laneplex', 'copywriter'): 1,\n",
       "  ('August 23rd', 'Streeter Inc.'): 1,\n",
       "  ('August 23rd', 'tech support'): 1,\n",
       "  ('Streeter Inc.', 'tech support'): 1},\n",
       " 7: {('August 20th', 'Canway'): 1,\n",
       "  ('August 20th', 'sales rep'): 1,\n",
       "  ('Canway', 'sales rep'): 1,\n",
       "  ('August 21st', 'Zathdrill'): 1,\n",
       "  ('August 21st', 'web developer'): 1,\n",
       "  ('Zathdrill', 'web developer'): 1,\n",
       "  ('August 22nd', 'Streeter Inc.'): 1,\n",
       "  ('August 22nd', 'graphic design'): 1,\n",
       "  ('Streeter Inc.', 'graphic design'): 1,\n",
       "  ('August 23rd', 'Laneplex'): 1,\n",
       "  ('August 23rd', 'tech support'): 1,\n",
       "  ('Laneplex', 'tech support'): 1},\n",
       " 8: {('April 13th', 'Adrian'): 1,\n",
       "  ('April 13th', 'Surgeon'): 1,\n",
       "  ('Adrian', 'Surgeon'): 1,\n",
       "  ('May 18th', 'Dale'): 1,\n",
       "  ('May 18th', 'Musician'): 1,\n",
       "  ('Dale', 'Musician'): 1,\n",
       "  ('June 14th', 'Harry'): 1,\n",
       "  ('June 14th', 'Lawyer'): 1,\n",
       "  ('Harry', 'Lawyer'): 1,\n",
       "  ('July 16th', 'Lamar'): 1,\n",
       "  ('July 16th', 'Architect'): 1,\n",
       "  ('Lamar', 'Architect'): 1},\n",
       " 9: {('7:00am', 'Janice'): 1,\n",
       "  ('Janice', 'Janice'): 1,\n",
       "  ('8:00am', 'Dustin'): 1,\n",
       "  ('Dustin', 'Dustin'): 1,\n",
       "  ('9:00am', 'Faye'): 1,\n",
       "  ('Faye', 'Faye'): 1,\n",
       "  ('10:00am', 'Kelly'): 1,\n",
       "  ('Kelly', 'Kelly'): 1},\n",
       " 10: {('$550', 'Honwa'): 1,\n",
       "  ('$550', 'Tela G5'): 1,\n",
       "  ('Honwa', 'Tela G5'): 1,\n",
       "  ('$575', 'Cober'): 1,\n",
       "  ('$575', 'DM-5000'): 1,\n",
       "  ('Cober', 'DM-5000'): 1,\n",
       "  ('$600', 'Banion'): 1,\n",
       "  ('$600', 'FC-520'): 1,\n",
       "  ('Banion', 'FC-520'): 1,\n",
       "  ('$625', 'Dayero'): 1,\n",
       "  ('$625', 'G-290'): 1,\n",
       "  ('Dayero', 'G-290'): 1},\n",
       " 11: {('$550', 'No Data'): 1,\n",
       "  ('No Data', 'No Data'): 1,\n",
       "  ('$575', 'No Data'): 1,\n",
       "  ('$600', 'No Data'): 1,\n",
       "  ('$625', 'No Data'): 1},\n",
       " 12: {('first', 'Splat Squad'): 1,\n",
       "  ('first', 'yellow'): 1,\n",
       "  ('Splat Squad', 'yellow'): 1,\n",
       "  ('second', 'Night Ninjas'): 1,\n",
       "  ('second', 'red'): 1,\n",
       "  ('Night Ninjas', 'red'): 1,\n",
       "  ('third', 'Oil Crew'): 1,\n",
       "  ('third', 'orange'): 1,\n",
       "  ('Oil Crew', 'orange'): 1,\n",
       "  ('fourth', 'Target Bombs'): 1,\n",
       "  ('fourth', 'purple'): 1,\n",
       "  ('Target Bombs', 'purple'): 1},\n",
       " 13: {('$3.30', 'No Data'): 1,\n",
       "  ('No Data', 'No Data'): 1,\n",
       "  ('$3.40', 'No Data'): 1,\n",
       "  ('$3.50', 'No Data'): 1,\n",
       "  ('$3.60', 'No Data'): 1},\n",
       " 14: {('135 ft', 'Flying Eyes'): 1,\n",
       "  ('135 ft', 'catapult'): 1,\n",
       "  ('Flying Eyes', 'catapult'): 1,\n",
       "  ('150 ft', 'Russets'): 1,\n",
       "  ('150 ft', 'trebuchet'): 1,\n",
       "  ('Russets', 'trebuchet'): 1,\n",
       "  ('165 ft', 'Spud Chuckers'): 1,\n",
       "  ('165 ft', 'slingshot'): 1,\n",
       "  ('Spud Chuckers', 'slingshot'): 1,\n",
       "  ('180 ft', 'Yukon Bolds'): 1,\n",
       "  ('180 ft', 'hybrid sling'): 1,\n",
       "  ('Yukon Bolds', 'hybrid sling'): 1},\n",
       " 15: {('1525', 'Amari Michi'): 1,\n",
       "  ('1525', 'Nakai'): 1,\n",
       "  ('Amari Michi', 'Nakai'): 1,\n",
       "  ('1585', 'Matsuno Okura'): 1,\n",
       "  ('1585', 'Niwa'): 1,\n",
       "  ('Matsuno Okura', 'Niwa'): 1,\n",
       "  ('1645', 'Takahashi'): 1,\n",
       "  ('1645', 'Uchida'): 1,\n",
       "  ('Takahashi', 'Uchida'): 1,\n",
       "  ('1705', 'Yamasaki'): 1,\n",
       "  ('1705', 'Usui'): 1,\n",
       "  ('Yamasaki', 'Usui'): 1},\n",
       " 16: {('1897', 'Zhale'): 1,\n",
       "  ('1897', 'Orange Sky'): 1,\n",
       "  ('Zhale', 'Orange Sky'): 1,\n",
       "  ('1905', 'Turxar'): 1,\n",
       "  ('1905', 'Willow Bend'): 1,\n",
       "  ('Turxar', 'Willow Bend'): 1,\n",
       "  ('1913', 'Verturu'): 1,\n",
       "  ('1913', 'Hoxley Hills'): 1,\n",
       "  ('Verturu', 'Hoxley Hills'): 1,\n",
       "  ('1921', 'Jenadije'): 1,\n",
       "  ('1921', 'Lost in Time'): 1,\n",
       "  ('Jenadije', 'Lost in Time'): 1,\n",
       "  ('1929', 'Ityer'): 1,\n",
       "  ('1929', 'Clockwork'): 1,\n",
       "  ('Ityer', 'Clockwork'): 1},\n",
       " 17: {('$400', 'Jack Jordan'): 1,\n",
       "  ('$400', 'toaster'): 1,\n",
       "  ('Jack Jordan', 'toaster'): 1,\n",
       "  ('$425', 'Lily Little'): 1,\n",
       "  ('$425', 'HD tv'): 1,\n",
       "  ('Lily Little', 'HD tv'): 1,\n",
       "  ('$450', 'Pam Powell'): 1,\n",
       "  ('$450', 'blu-ray player'): 1,\n",
       "  ('Pam Powell', 'blu-ray player'): 1,\n",
       "  ('$475', 'Ned Norris'): 1,\n",
       "  ('$475', 'juicer'): 1,\n",
       "  ('Ned Norris', 'juicer'): 1,\n",
       "  ('$500', 'Mitch Mayo'): 1,\n",
       "  ('$500', 'blender'): 1,\n",
       "  ('Mitch Mayo', 'blender'): 1},\n",
       " 18: {('12:01am', 'Toni'): 1,\n",
       "  ('12:01am', 'Blanchard'): 1,\n",
       "  ('Toni', 'Blanchard'): 1,\n",
       "  ('12:04am', 'Peggy'): 1,\n",
       "  ('12:04am', 'Ortega'): 1,\n",
       "  ('Peggy', 'Ortega'): 1,\n",
       "  ('12:07am', 'Susan'): 1,\n",
       "  ('12:07am', 'Vaughan'): 1,\n",
       "  ('Susan', 'Vaughan'): 1,\n",
       "  ('12:10am', 'Iris'): 1,\n",
       "  ('12:10am', 'Quinn'): 1,\n",
       "  ('Iris', 'Quinn'): 1,\n",
       "  ('12:13am', 'Luis'): 1,\n",
       "  ('12:13am', 'Estrada'): 1,\n",
       "  ('Luis', 'Estrada'): 1},\n",
       " 19: {('2', 'Yates'): 1,\n",
       "  ('2', '117'): 1,\n",
       "  ('Yates', '117'): 1,\n",
       "  ('3', 'Owens'): 1,\n",
       "  ('3', '203'): 1,\n",
       "  ('Owens', '203'): 1,\n",
       "  ('4', 'Hartman'): 1,\n",
       "  ('4', '320'): 1,\n",
       "  ('Hartman', '320'): 1,\n",
       "  ('5', 'Mr. Bray'): 1,\n",
       "  ('5', '209'): 1,\n",
       "  ('Mr. Bray', '209'): 1,\n",
       "  ('6', 'Navarro'): 1,\n",
       "  ('6', '124'): 1,\n",
       "  ('Navarro', '124'): 1},\n",
       " 20: {('23', 'Tau Beta Pi'): 1,\n",
       "  ('23', 'Wild Avenue'): 1,\n",
       "  ('Tau Beta Pi', 'Wild Avenue'): 1,\n",
       "  ('26', 'Tau Beta Pi'): 1,\n",
       "  ('26', 'Wild Avenue'): 1,\n",
       "  ('29', 'Alpha Beta Tau'): 1,\n",
       "  ('29', 'Kermit Lane'): 1,\n",
       "  ('Alpha Beta Tau', 'Kermit Lane'): 1,\n",
       "  ('32', 'Beta Pi Omega'): 1,\n",
       "  ('32', 'Park Street'): 1,\n",
       "  ('Beta Pi Omega', 'Park Street'): 1,\n",
       "  ('35', 'Lambda Alpha'): 1,\n",
       "  ('35', 'Russell Road'): 1,\n",
       "  ('Lambda Alpha', 'Russell Road'): 1},\n",
       " 21: {('**24**', 'Krista'): 1,\n",
       "  ('**24**', '4'): 1,\n",
       "  ('Krista', '4'): 1,\n",
       "  ('**27**', 'Nina'): 1,\n",
       "  ('**27**', '7'): 1,\n",
       "  ('Nina', '7'): 1,\n",
       "  ('**30**', 'Sonia'): 1,\n",
       "  ('**30**', '10'): 1,\n",
       "  ('Sonia', '10'): 1,\n",
       "  ('**33**', 'Tracey'): 1,\n",
       "  ('**33**', '12'): 1,\n",
       "  ('Tracey', '12'): 1,\n",
       "  ('**36**', 'Vicki'): 1,\n",
       "  ('**36**', '15'): 1,\n",
       "  ('Vicki', '15'): 1},\n",
       " 22: {('50', 'wedding'): 1,\n",
       "  ('wedding', 'wedding'): 1,\n",
       "  ('75', 'wedding'): 1,\n",
       "  ('100', 'wedding'): 1,\n",
       "  ('125', 'wedding'): 1,\n",
       "  ('150', 'Bridget, Flora, Oliver, Patrice'): 1,\n",
       "  ('Bridget, Flora, Oliver, Patrice', 'Bridget, Flora, Oliver, Patrice'): 1},\n",
       " 23: {('4', 'Marco'): 1,\n",
       "  ('4', 'Golf'): 1,\n",
       "  ('Marco', 'Golf'): 1,\n",
       "  ('11', 'Marco'): 1,\n",
       "  ('11', 'Hockey'): 1,\n",
       "  ('Marco', 'Hockey'): 1,\n",
       "  ('18', 'Rudolph'): 1,\n",
       "  ('18', 'Soccer'): 1,\n",
       "  ('Rudolph', 'Soccer'): 1,\n",
       "  ('25', 'Nathaniel'): 1,\n",
       "  ('25', 'Basketball'): 1,\n",
       "  ('Nathaniel', 'Basketball'): 1,\n",
       "  ('32', 'Eduardo'): 1,\n",
       "  ('32', 'Lacrosse'): 1,\n",
       "  ('Eduardo', 'Lacrosse'): 1},\n",
       " 24: {('June 10', 'Vernon'): 1,\n",
       "  ('June 10', 'Kent'): 1,\n",
       "  ('Vernon', 'Kent'): 1,\n",
       "  ('June 11', 'Vernon'): 1,\n",
       "  ('June 11', 'Kent'): 1,\n",
       "  ('June 12', 'Vernon'): 1,\n",
       "  ('June 12', 'Kent'): 1,\n",
       "  ('June 13', 'Vernon'): 1,\n",
       "  ('June 13', 'Kent'): 1,\n",
       "  ('June 14', 'Ira'): 1,\n",
       "  ('June 14', 'Buttonwillow'): 1,\n",
       "  ('Ira', 'Buttonwillow'): 1,\n",
       "  ('June 14', 'Luke'): 1,\n",
       "  ('June 14', 'Coulterville'): 1,\n",
       "  ('Luke', 'Coulterville'): 1,\n",
       "  ('June 14', 'Nathan'): 1,\n",
       "  ('June 14', 'Farley'): 1,\n",
       "  ('Nathan', 'Farley'): 1,\n",
       "  ('June 14', 'Stanley'): 1,\n",
       "  ('June 14', 'Hardwick'): 1,\n",
       "  ('Stanley', 'Hardwick'): 1,\n",
       "  ('June 14', 'Vernon'): 1,\n",
       "  ('June 14', 'Kent'): 1},\n",
       " 25: {('$45', 'Gina'): 1,\n",
       "  ('$45', 'chalkhill'): 1,\n",
       "  ('Gina', 'chalkhill'): 1,\n",
       "  ('$60', 'Heather'): 1,\n",
       "  ('$60', 'grayling'): 1,\n",
       "  ('Heather', 'grayling'): 1,\n",
       "  ('$75', 'Marlene'): 1,\n",
       "  ('$75', 'peppered'): 1,\n",
       "  ('Marlene', 'peppered'): 1,\n",
       "  ('$90', 'Phillip'): 1,\n",
       "  ('$90', 'swallowtail'): 1,\n",
       "  ('Phillip', 'swallowtail'): 1,\n",
       "  ('$105', 'Victor'): 1,\n",
       "  ('$105', 'torturix'): 1,\n",
       "  ('Victor', 'torturix'): 1}}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup_table_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gemini = pd.read_csv(r\"../../GridPuzzle_gemini.csv\")\n",
    "df_gemini = df_gemini[df_gemini[\"id\"].isin([13, 821, 687, 718, 350, 1084, 1677, 1437, 1597, 1935, 1020, 1495, 2893, 2208, 2700, 2788, 3720, 3936, 3642, 3659, 4465, 4658, 4087, 4861, 4140, 4464, 4848, 3954, 5740, 5675, 5212, 4976, 5889, 5659, 5325, 5585, 6843, 6292, 6045, 6432, 6823, 5923, 6778, 6997, 7685, 7865, 7183, 7389, 7866, 7188, 7837, 8712, 7959, 8356, 8142, 7993, 8519, 9106, 9719, 9139, 9487, 8853, 8988, 9043, 9497, 9596, 9965, 10037, 10380, 10100, 10450, 10440, 9976, 10251, 11592, 11097, 11022, 11603, 11546, 11748, 11263, 11367, 11407, 11304, 11321, 11923, 12703, 12356, 12535, 12172, 12133, 12395, 12818, 13003, 12900, 13407, 13198, 12782, 13051, 12980])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>Mistral-7b</th>\n",
       "      <th>Llama-13b</th>\n",
       "      <th>gemini-pro</th>\n",
       "      <th>gpt-4-turbo</th>\n",
       "      <th>Claude-3</th>\n",
       "      <th>gemini-2.0-flash-exp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>341.json</td>\n",
       "      <td>13</td>\n",
       "      <td>A group of friends has decided to try several ...</td>\n",
       "      <td>3 | Raymond | vegan\\n5 | Wesley | gluten-free\\...</td>\n",
       "      <td>|------------|------------|\\n | 3 | Raymond |\\...</td>\n",
       "      <td>Let's solve the puzzle step by step!\\n \\n Step...</td>\n",
       "      <td>Step-by-step solution:\\n 1. From clue 3, we kn...</td>\n",
       "      <td>Step-by-step solution:\\n \\n 1. From Clue 3, we...</td>\n",
       "      <td>Step-by-step solution:\\n 1. From clue 3, we kn...</td>\n",
       "      <td>Step-by-step solution:\\n1. From clue 3, Stuart...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>341.json</td>\n",
       "      <td>821</td>\n",
       "      <td>Badgerville High's football team has selected ...</td>\n",
       "      <td>3 | Gordon Gregg | running back\\n4 | Kurt Koch...</td>\n",
       "      <td>|---------------------------------------------...</td>\n",
       "      <td>Let's break down the clues and solve the puzzl...</td>\n",
       "      <td>Step-by-step solution:\\n 1. From clue 4, we kn...</td>\n",
       "      <td>Step-by-step solution:\\n \\n Step 1: Analyze Cl...</td>\n",
       "      <td>Step-by-step solution:\\n \\n 1. From clue 3, we...</td>\n",
       "      <td>Step-by-step solution:\\n\\n1.  From clue 3, Kur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>341.json</td>\n",
       "      <td>687</td>\n",
       "      <td>Using only the clues below, match the months t...</td>\n",
       "      <td>January | Rosie | shih tzu\\nFebruary | Lydia |...</td>\n",
       "      <td>| January | Kayla | schnauzer\\n | February | L...</td>\n",
       "      <td>Let's break down the clues step by step:\\n \\n ...</td>\n",
       "      <td>Step-by-step solution:\\n \\n 1. From clue 1, we...</td>\n",
       "      <td>Step-by-step solution:\\n \\n 1. From Clue 1, we...</td>\n",
       "      <td>Step-by-step solution:\\n 1. From clue 1, we kn...</td>\n",
       "      <td>Step-by-step solution:\\n1. From clue 1, we kno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>341.json</td>\n",
       "      <td>718</td>\n",
       "      <td>A number of new space probes were launched thi...</td>\n",
       "      <td>January | Viria Mari | Mercury\\nFebruary | Iun...</td>\n",
       "      <td>Step-by-step solution:\\n From clue 1, we know ...</td>\n",
       "      <td>Let's break down the clues and solve the puzzl...</td>\n",
       "      <td>Step-by-step solution:\\n 1. From clue 1, we kn...</td>\n",
       "      <td>Step-by-step solution:\\n \\n 1. From Clue 2, we...</td>\n",
       "      <td>Step-by-step solution:\\n \\n 1. From clue 2, we...</td>\n",
       "      <td>Step-by-step solution:\\n1. From clue 2, Disty ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>341.json</td>\n",
       "      <td>350</td>\n",
       "      <td>The Albion Amateur Astronomy Club proudly proc...</td>\n",
       "      <td>2008 | Jack Ingram | Peinope\\n2009 | Hal Grego...</td>\n",
       "      <td>| 2008 | Jack Ingram | Crecci or Ken Jones |\\n...</td>\n",
       "      <td>Let's break down the clues step by step:\\n \\n ...</td>\n",
       "      <td>Step-by-step solution:\\n 1. From clue 1, we kn...</td>\n",
       "      <td>Step-by-step solution:\\n \\n 1. From clue 1, we...</td>\n",
       "      <td>Step-by-step solution:\\n \\n 1. From clue 1, we...</td>\n",
       "      <td>Step-by-step solution:\\n1. From clue 1, the co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         key   id                                           question  \\\n",
       "6   341.json   13  A group of friends has decided to try several ...   \n",
       "9   341.json  821  Badgerville High's football team has selected ...   \n",
       "10  341.json  687  Using only the clues below, match the months t...   \n",
       "12  341.json  718  A number of new space probes were launched thi...   \n",
       "19  341.json  350  The Albion Amateur Astronomy Club proudly proc...   \n",
       "\n",
       "                                               answer  \\\n",
       "6   3 | Raymond | vegan\\n5 | Wesley | gluten-free\\...   \n",
       "9   3 | Gordon Gregg | running back\\n4 | Kurt Koch...   \n",
       "10  January | Rosie | shih tzu\\nFebruary | Lydia |...   \n",
       "12  January | Viria Mari | Mercury\\nFebruary | Iun...   \n",
       "19  2008 | Jack Ingram | Peinope\\n2009 | Hal Grego...   \n",
       "\n",
       "                                           Mistral-7b  \\\n",
       "6   |------------|------------|\\n | 3 | Raymond |\\...   \n",
       "9   |---------------------------------------------...   \n",
       "10  | January | Kayla | schnauzer\\n | February | L...   \n",
       "12  Step-by-step solution:\\n From clue 1, we know ...   \n",
       "19  | 2008 | Jack Ingram | Crecci or Ken Jones |\\n...   \n",
       "\n",
       "                                            Llama-13b  \\\n",
       "6   Let's solve the puzzle step by step!\\n \\n Step...   \n",
       "9   Let's break down the clues and solve the puzzl...   \n",
       "10  Let's break down the clues step by step:\\n \\n ...   \n",
       "12  Let's break down the clues and solve the puzzl...   \n",
       "19  Let's break down the clues step by step:\\n \\n ...   \n",
       "\n",
       "                                           gemini-pro  \\\n",
       "6   Step-by-step solution:\\n 1. From clue 3, we kn...   \n",
       "9   Step-by-step solution:\\n 1. From clue 4, we kn...   \n",
       "10  Step-by-step solution:\\n \\n 1. From clue 1, we...   \n",
       "12  Step-by-step solution:\\n 1. From clue 1, we kn...   \n",
       "19  Step-by-step solution:\\n 1. From clue 1, we kn...   \n",
       "\n",
       "                                          gpt-4-turbo  \\\n",
       "6   Step-by-step solution:\\n \\n 1. From Clue 3, we...   \n",
       "9   Step-by-step solution:\\n \\n Step 1: Analyze Cl...   \n",
       "10  Step-by-step solution:\\n \\n 1. From Clue 1, we...   \n",
       "12  Step-by-step solution:\\n \\n 1. From Clue 2, we...   \n",
       "19  Step-by-step solution:\\n \\n 1. From clue 1, we...   \n",
       "\n",
       "                                             Claude-3  \\\n",
       "6   Step-by-step solution:\\n 1. From clue 3, we kn...   \n",
       "9   Step-by-step solution:\\n \\n 1. From clue 3, we...   \n",
       "10  Step-by-step solution:\\n 1. From clue 1, we kn...   \n",
       "12  Step-by-step solution:\\n \\n 1. From clue 2, we...   \n",
       "19  Step-by-step solution:\\n \\n 1. From clue 1, we...   \n",
       "\n",
       "                                 gemini-2.0-flash-exp  \n",
       "6   Step-by-step solution:\\n1. From clue 3, Stuart...  \n",
       "9   Step-by-step solution:\\n\\n1.  From clue 3, Kur...  \n",
       "10  Step-by-step solution:\\n1. From clue 1, we kno...  \n",
       "12  Step-by-step solution:\\n1. From clue 2, Disty ...  \n",
       "19  Step-by-step solution:\\n1. From clue 1, the co...  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gemini.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>Mistral-7b</th>\n",
       "      <th>Llama-13b</th>\n",
       "      <th>gemini-pro</th>\n",
       "      <th>gpt-4-turbo</th>\n",
       "      <th>Claude-3</th>\n",
       "      <th>actual_question</th>\n",
       "      <th>Clues</th>\n",
       "      <th>categories</th>\n",
       "      <th>number_of_clues</th>\n",
       "      <th>number_categories</th>\n",
       "      <th>category_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>341.json</td>\n",
       "      <td>13</td>\n",
       "      <td>A group of friends has decided to try several ...</td>\n",
       "      <td>3 | Raymond | vegan\\n5 | Wesley | gluten-free\\...</td>\n",
       "      <td>|------------|------------|\\n | 3 | Raymond |\\...</td>\n",
       "      <td>Let's solve the puzzle step by step!\\n \\n Step...</td>\n",
       "      <td>Step-by-step solution:\\n 1. From clue 3, we kn...</td>\n",
       "      <td>Step-by-step solution:\\n \\n 1. From Clue 3, we...</td>\n",
       "      <td>Step-by-step solution:\\n 1. From clue 3, we kn...</td>\n",
       "      <td>A group of friends has decided to try several ...</td>\n",
       "      <td>Clues:\\n1. Stuart lost 4 more pounds than the ...</td>\n",
       "      <td>pounds lost : 3, 5, 7, 9.\\nnames : Raymond, St...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>lost names diets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>341.json</td>\n",
       "      <td>821</td>\n",
       "      <td>Badgerville High's football team has selected ...</td>\n",
       "      <td>3 | Gordon Gregg | running back\\n4 | Kurt Koch...</td>\n",
       "      <td>|---------------------------------------------...</td>\n",
       "      <td>Let's break down the clues and solve the puzzl...</td>\n",
       "      <td>Step-by-step solution:\\n 1. From clue 4, we kn...</td>\n",
       "      <td>Step-by-step solution:\\n \\n Step 1: Analyze Cl...</td>\n",
       "      <td>Step-by-step solution:\\n \\n 1. From clue 3, we...</td>\n",
       "      <td>Badgerville High's football team has selected ...</td>\n",
       "      <td>Clues:\\n1. Edward Ellis played 2  more games t...</td>\n",
       "      <td>games : 3, 4, 5, 6.\\nplayers : Edward Ellis, G...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>games players positions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>341.json</td>\n",
       "      <td>687</td>\n",
       "      <td>Using only the clues below, match the months t...</td>\n",
       "      <td>January | Rosie | shih tzu\\nFebruary | Lydia |...</td>\n",
       "      <td>| January | Kayla | schnauzer\\n | February | L...</td>\n",
       "      <td>Let's break down the clues step by step:\\n \\n ...</td>\n",
       "      <td>Step-by-step solution:\\n \\n 1. From clue 1, we...</td>\n",
       "      <td>Step-by-step solution:\\n \\n 1. From Clue 1, we...</td>\n",
       "      <td>Step-by-step solution:\\n 1. From clue 1, we kn...</td>\n",
       "      <td>Using only the clues below, match the months t...</td>\n",
       "      <td>Clues:\\n1. Violet's pet is the boxer.\\n2. Kayl...</td>\n",
       "      <td>months : January, February, March, April.\\nown...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>months owners breeds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>341.json</td>\n",
       "      <td>718</td>\n",
       "      <td>A number of new space probes were launched thi...</td>\n",
       "      <td>January | Viria Mari | Mercury\\nFebruary | Iun...</td>\n",
       "      <td>Step-by-step solution:\\n From clue 1, we know ...</td>\n",
       "      <td>Let's break down the clues and solve the puzzl...</td>\n",
       "      <td>Step-by-step solution:\\n 1. From clue 1, we kn...</td>\n",
       "      <td>Step-by-step solution:\\n \\n 1. From Clue 2, we...</td>\n",
       "      <td>Step-by-step solution:\\n \\n 1. From clue 2, we...</td>\n",
       "      <td>A number of new space probes were launched thi...</td>\n",
       "      <td>Clues:\\n1. The Iunsel launched sometime after ...</td>\n",
       "      <td>months : January, February, March, April.\\npro...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>months probes destinations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>341.json</td>\n",
       "      <td>350</td>\n",
       "      <td>The Albion Amateur Astronomy Club proudly proc...</td>\n",
       "      <td>2008 | Jack Ingram | Peinope\\n2009 | Hal Grego...</td>\n",
       "      <td>| 2008 | Jack Ingram | Crecci or Ken Jones |\\n...</td>\n",
       "      <td>Let's break down the clues step by step:\\n \\n ...</td>\n",
       "      <td>Step-by-step solution:\\n 1. From clue 1, we kn...</td>\n",
       "      <td>Step-by-step solution:\\n \\n 1. From clue 1, we...</td>\n",
       "      <td>Step-by-step solution:\\n \\n 1. From clue 1, we...</td>\n",
       "      <td>The Albion Amateur Astronomy Club proudly proc...</td>\n",
       "      <td>Clues:\\n1. The one discovered in 2009 is Caspu...</td>\n",
       "      <td>years : 2008, 2009, 2010, 2011.\\nastronomers :...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>years astronomers comets</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         key   id                                           question  \\\n",
       "6   341.json   13  A group of friends has decided to try several ...   \n",
       "9   341.json  821  Badgerville High's football team has selected ...   \n",
       "10  341.json  687  Using only the clues below, match the months t...   \n",
       "12  341.json  718  A number of new space probes were launched thi...   \n",
       "19  341.json  350  The Albion Amateur Astronomy Club proudly proc...   \n",
       "\n",
       "                                               answer  \\\n",
       "6   3 | Raymond | vegan\\n5 | Wesley | gluten-free\\...   \n",
       "9   3 | Gordon Gregg | running back\\n4 | Kurt Koch...   \n",
       "10  January | Rosie | shih tzu\\nFebruary | Lydia |...   \n",
       "12  January | Viria Mari | Mercury\\nFebruary | Iun...   \n",
       "19  2008 | Jack Ingram | Peinope\\n2009 | Hal Grego...   \n",
       "\n",
       "                                           Mistral-7b  \\\n",
       "6   |------------|------------|\\n | 3 | Raymond |\\...   \n",
       "9   |---------------------------------------------...   \n",
       "10  | January | Kayla | schnauzer\\n | February | L...   \n",
       "12  Step-by-step solution:\\n From clue 1, we know ...   \n",
       "19  | 2008 | Jack Ingram | Crecci or Ken Jones |\\n...   \n",
       "\n",
       "                                            Llama-13b  \\\n",
       "6   Let's solve the puzzle step by step!\\n \\n Step...   \n",
       "9   Let's break down the clues and solve the puzzl...   \n",
       "10  Let's break down the clues step by step:\\n \\n ...   \n",
       "12  Let's break down the clues and solve the puzzl...   \n",
       "19  Let's break down the clues step by step:\\n \\n ...   \n",
       "\n",
       "                                           gemini-pro  \\\n",
       "6   Step-by-step solution:\\n 1. From clue 3, we kn...   \n",
       "9   Step-by-step solution:\\n 1. From clue 4, we kn...   \n",
       "10  Step-by-step solution:\\n \\n 1. From clue 1, we...   \n",
       "12  Step-by-step solution:\\n 1. From clue 1, we kn...   \n",
       "19  Step-by-step solution:\\n 1. From clue 1, we kn...   \n",
       "\n",
       "                                          gpt-4-turbo  \\\n",
       "6   Step-by-step solution:\\n \\n 1. From Clue 3, we...   \n",
       "9   Step-by-step solution:\\n \\n Step 1: Analyze Cl...   \n",
       "10  Step-by-step solution:\\n \\n 1. From Clue 1, we...   \n",
       "12  Step-by-step solution:\\n \\n 1. From Clue 2, we...   \n",
       "19  Step-by-step solution:\\n \\n 1. From clue 1, we...   \n",
       "\n",
       "                                             Claude-3  \\\n",
       "6   Step-by-step solution:\\n 1. From clue 3, we kn...   \n",
       "9   Step-by-step solution:\\n \\n 1. From clue 3, we...   \n",
       "10  Step-by-step solution:\\n 1. From clue 1, we kn...   \n",
       "12  Step-by-step solution:\\n \\n 1. From clue 2, we...   \n",
       "19  Step-by-step solution:\\n \\n 1. From clue 1, we...   \n",
       "\n",
       "                                      actual_question  \\\n",
       "6   A group of friends has decided to try several ...   \n",
       "9   Badgerville High's football team has selected ...   \n",
       "10  Using only the clues below, match the months t...   \n",
       "12  A number of new space probes were launched thi...   \n",
       "19  The Albion Amateur Astronomy Club proudly proc...   \n",
       "\n",
       "                                                Clues  \\\n",
       "6   Clues:\\n1. Stuart lost 4 more pounds than the ...   \n",
       "9   Clues:\\n1. Edward Ellis played 2  more games t...   \n",
       "10  Clues:\\n1. Violet's pet is the boxer.\\n2. Kayl...   \n",
       "12  Clues:\\n1. The Iunsel launched sometime after ...   \n",
       "19  Clues:\\n1. The one discovered in 2009 is Caspu...   \n",
       "\n",
       "                                           categories  number_of_clues  \\\n",
       "6   pounds lost : 3, 5, 7, 9.\\nnames : Raymond, St...                5   \n",
       "9   games : 3, 4, 5, 6.\\nplayers : Edward Ellis, G...                5   \n",
       "10  months : January, February, March, April.\\nown...                4   \n",
       "12  months : January, February, March, April.\\npro...                5   \n",
       "19  years : 2008, 2009, 2010, 2011.\\nastronomers :...                5   \n",
       "\n",
       "    number_categories              category_names  \n",
       "6                   3            lost names diets  \n",
       "9                   3     games players positions  \n",
       "10                  3        months owners breeds  \n",
       "12                  3  months probes destinations  \n",
       "19                  3    years astronomers comets  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([  6,   9,  10,  12,  19,  22,  24,  28,  30,  33,  36,  37,  45,  46,\n",
       "        49,  54,  67,  71,  76,  78,  80,  81,  83,  88,  90,  95,  96,  98,\n",
       "       100, 101, 103, 106, 113, 114, 117, 119, 120, 121, 129, 132, 134, 137,\n",
       "       139, 146, 150, 151, 152, 153, 154, 157, 158, 160, 164, 165, 170, 177,\n",
       "       179, 183, 184, 185, 186, 189, 192, 193, 195, 197, 202, 203, 210, 213,\n",
       "       215, 216, 217, 218, 220, 222, 225, 226, 227, 230, 231, 232, 234, 235,\n",
       "       239, 242, 245, 247, 249, 250, 251, 258, 260, 264, 266, 267, 268, 269,\n",
       "       271, 273],\n",
       "      dtype='int64')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([  6,   9,  10,  12,  19,  22,  24,  28,  30,  33,  36,  37,  45,  46,\n",
       "        49,  54,  67,  71,  76,  78,  80,  81,  83,  88,  90,  95,  96,  98,\n",
       "       100, 101, 103, 106, 113, 114, 117, 119, 120, 121, 129, 132, 134, 137,\n",
       "       139, 146, 150, 151, 152, 153, 154, 157, 158, 160, 164, 165, 170, 177,\n",
       "       179, 183, 184, 185, 186, 189, 192, 193, 195, 197, 202, 203, 210, 213,\n",
       "       215, 216, 217, 218, 220, 222, 225, 226, 227, 230, 231, 232, 234, 235,\n",
       "       239, 242, 245, 247, 249, 250, 251, 258, 260, 264, 266, 267, 268, 269,\n",
       "       271, 273],\n",
       "      dtype='int64')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gemini.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "('Step-by-step solution:\\n'\n",
      " '1. From clue 3, Stuart lost 7 lbs.\\n'\n",
      " '2. From clue 5, Wesley lost 5 lbs.\\n'\n",
      " '3. From clue 1, Stuart lost 4 more pounds than the person who used the vegan '\n",
      " 'diet. Since Stuart lost 7 lbs, the vegan diet resulted in a 3 lb loss.\\n'\n",
      " '4. From clue 2, the gluten-free dieter lost 2 more pounds than Raymond. This '\n",
      " 'means Raymond did not lose 7 lbs (Stuart lost 7 lbs) and Raymond did not '\n",
      " 'lose 5 lbs (Wesley lost 5 lbs) and Raymond did not lose 3 lbs (Vegan diet '\n",
      " 'resulted in 3 lbs). So, Raymond lost 9 lbs, which makes the gluten-free diet '\n",
      " 'result in 9+2 = 11 lbs, which is impossible, so Raymond must have lost less '\n",
      " 'than 7, and the gluten free dieter did as well. So this leaves 3 lbs lost, '\n",
      " 'and 5 lbs lost for Raymond. Because the vegan dieter lost 3 lbs, Raymond '\n",
      " 'lost 5 lbs, which means the gluten-free dieter lost 7 lbs.\\n'\n",
      " '5. From clue 4, the dieter who lost 7 lbs (Stuart) is either the person who '\n",
      " 'used the caveman diet or Raymond. Since the gluten-free dieter lost 7 lbs, '\n",
      " 'the dieter who lost 7 lbs could not have been Raymond, so Stuart used the '\n",
      " 'gluten-free diet.\\n'\n",
      " '6. Since Stuart used the gluten-free diet, Raymond did not, and Wesley did '\n",
      " 'not use the vegan diet, and Stuart did not use the vegan diet, and Raymond '\n",
      " 'did not use the gluten-free diet, and Wesley did not use the gluten-free '\n",
      " 'diet, Tom must have used the vegan diet. Raymond lost 5 lbs and Wesley lost '\n",
      " '5 lbs, but each person lost a different amount, so this cannot be true. '\n",
      " \"Let's re-evaluate\\n\"\n",
      " '1. Stuart lost 7 lbs\\n'\n",
      " '2. Wesley lost 5 lbs\\n'\n",
      " '3. Vegan diet resulted in 3 lb loss\\n'\n",
      " \"4. Gluten-free dieter lost 2 more lbs than Raymond, so Raymond can't have \"\n",
      " \"7lbs. Also, since Raymond isn't vegan, gluten-free, or 7 lbs, Raymond either \"\n",
      " 'lost 9 or 5 lbs. The gluten-free dieter lost 2 more, which would be 11 and 7 '\n",
      " \"respectively. 11 isn't valid, so we need to have Raymond lost 5 and the \"\n",
      " 'gluten-free lost 7\\n'\n",
      " '5. Stuart lost 7 lbs, and the 7 lb dieter is either caveman or Raymond. '\n",
      " 'Since the gluten-free dieter lost 7 lbs, Stuart is the gluten-free. Since '\n",
      " 'Wesley lost 5 lbs, and Raymond lost 5 lbs, this cannot be true.\\n'\n",
      " '\\n'\n",
      " \"Ok, let's try this\\n\"\n",
      " '1. Stuart lost 7\\n'\n",
      " '2. Wesley lost 5\\n'\n",
      " '3. Vegan lost 3\\n'\n",
      " '4. Gluten-free = Raymond + 2\\n'\n",
      " '5. Stuart (7) = caveman or Raymond\\n'\n",
      " '\\n'\n",
      " \"From 4, since Raymond + 2, Raymond can't have 7 (Stuart), 3(vegan) or \"\n",
      " '5(wesley). So Raymond must have lost 9 lbs, and gluten-free lost 11, which '\n",
      " 'is impossible. We need a number lower than 7, so that Raymond + 2 results in '\n",
      " 'something under 10. Vegan = 3, Wesley = 5, Stuart = 7. So Raymond + 2 = '\n",
      " 'someone under 10.\\n'\n",
      " 'This means the Raymond can only have 3 or 5 pounds lost\\n'\n",
      " 'Stuart lost 4 more pounds than the vegan (3lbs), thus Stuart lost 7.\\n'\n",
      " '\\n'\n",
      " 'Stuart is either Raymond or caveman. Raymond cannot be 7, Raymond cannot be '\n",
      " '3, Raymond cannot be 5. Raymond is 9! Gluten free (Raymond + 2 = 11, '\n",
      " 'impossible).\\n'\n",
      " '\\n'\n",
      " \"There's an error in the instructions, as 3 people lost different amounts, \"\n",
      " 'only to have Raymond and Wesley lose the same amount. The clues above '\n",
      " 'conflict with the original premise. Since Wesley lost 5lbs, and Raymond lost '\n",
      " '5lbs, they lost the same amount.\\n'\n",
      " '\\n'\n",
      " 'Final Answer:\\n'\n",
      " '3 | Tom | vegan\\n'\n",
      " '5 | Wesley | caveman\\n'\n",
      " '7 | Stuart | gluten-free\\n'\n",
      " '9 | Raymond | dairy-free\\n')\n",
      "\n",
      "\n",
      "('3 | Raymond | vegan\\n'\n",
      " '5 | Wesley | gluten-free\\n'\n",
      " '7 | Stuart | caveman\\n'\n",
      " '9 | Tom | dairy-free')\n",
      "9\n",
      "('Step-by-step solution:\\n'\n",
      " '\\n'\n",
      " '1.  From clue 3, Kurt Koch is the defensive end.\\n'\n",
      " '2.  From clue 1, Edward Ellis played 2 more games than the defensive end '\n",
      " '(Kurt Koch).\\n'\n",
      " '3.  From clue 4, Kurt Koch played 2 fewer games than the placekicker. '\n",
      " 'Therefore, Edward Ellis and the placekicker played the same number of '\n",
      " 'games.\\n'\n",
      " '4.  Since Kurt Koch is the defensive end, and Edward Ellis played 2 more '\n",
      " 'games, the number of games Kurt Koch played could be 3 or 4. If Kurt Koch '\n",
      " 'played 5 games, then Edward Ellis would have played 7, which is impossible.\\n'\n",
      " '5.  Clue 4 tells us Kurt Koch played 2 fewer games than the placekicker. '\n",
      " 'This means the placekicker could have played either 5 or 6 games.\\n'\n",
      " '6.  From clue 2, the person who played in 5 games last season is the '\n",
      " 'quarterback. Thus, the placekicker cannot be the quarterback, so the '\n",
      " 'placekicker must have played 6 games, and Kurt Koch (defensive end) played 4 '\n",
      " 'games, and Edward Ellis played 6 games.\\n'\n",
      " '7.  Since Kurt Koch played 4 games, Edward Ellis cannot have played 4 '\n",
      " 'games.\\n'\n",
      " '8.  We now have the following: Kurt Koch - 4 games - defensive end; Edward '\n",
      " 'Ellis - 6 games - not placekicker.\\n'\n",
      " '9.  The quarterback played 5 games. Since Edward Ellis played 6 games, he '\n",
      " 'cannot be the quarterback. Therefore, Edward Ellis is the placekicker.\\n'\n",
      " '10. Clue 5 states the running back is either Gordon Gregg or the person who '\n",
      " 'played in 4 games last season. Kurt Koch played in 4 games and is the '\n",
      " 'defensive end. Therefore, Gordon Gregg is the running back.\\n'\n",
      " '11. Therefore, Jack Jensen must be the quarterback. He plays 5 games.\\n'\n",
      " '12. Gordon Gregg plays the remaining number of games, which is 3, and he is '\n",
      " 'the running back.\\n'\n",
      " '\\n'\n",
      " 'Final Answer:\\n'\n",
      " '\\n'\n",
      " '3 | Gordon Gregg | running back\\n'\n",
      " '4 | Kurt Koch | defensive end\\n'\n",
      " '5 | Jack Jensen | quarterback\\n'\n",
      " '6 | Edward Ellis | placekicker\\n')\n",
      "\n",
      "\n",
      "('3 | Gordon Gregg | running back\\n'\n",
      " '4 | Kurt Koch | defensive end\\n'\n",
      " '5 | Jack Jensen | quarterback\\n'\n",
      " '6 | Edward Ellis | placekicker')\n"
     ]
    }
   ],
   "source": [
    "hard_correct = []\n",
    "soft_correct = []\n",
    "for id_,row in df.iterrows():\n",
    "    \n",
    "    print(id_)\n",
    "    if id_ >=25:\n",
    "        break\n",
    "    \n",
    "    pprint(df_gemini[\"gemini-2.0-flash-exp\"].loc[id_])\n",
    "   \n",
    "\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "    pprint(row[\"answer\"])\n",
    "\n",
    "    if input() == \"1\":\n",
    "        hard_correct.append(1)\n",
    "    \n",
    "    else:\n",
    "        hard_correct.append(0)\n",
    "\n",
    "    \n",
    "    if input() == \"1\":\n",
    "        soft_correct.append(1)\n",
    "    \n",
    "    else:\n",
    "        soft_correct.append(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Step-by-step solution:\\n'\n",
      " '1. From clue 3, Stuart lost 7 lbs.\\n'\n",
      " '2. From clue 5, Wesley lost 5 lbs.\\n'\n",
      " '3. From clue 1, Stuart lost 4 more pounds than the person who used the vegan '\n",
      " 'diet. Since Stuart lost 7 lbs, the vegan diet resulted in a 3 lb loss.\\n'\n",
      " '4. From clue 2, the gluten-free dieter lost 2 more pounds than Raymond. This '\n",
      " 'means Raymond did not lose 7 lbs (Stuart lost 7 lbs) and Raymond did not '\n",
      " 'lose 5 lbs (Wesley lost 5 lbs) and Raymond did not lose 3 lbs (Vegan diet '\n",
      " 'resulted in 3 lbs). So, Raymond lost 9 lbs, which makes the gluten-free diet '\n",
      " 'result in 9+2 = 11 lbs, which is impossible, so Raymond must have lost less '\n",
      " 'than 7, and the gluten free dieter did as well. So this leaves 3 lbs lost, '\n",
      " 'and 5 lbs lost for Raymond. Because the vegan dieter lost 3 lbs, Raymond '\n",
      " 'lost 5 lbs, which means the gluten-free dieter lost 7 lbs.\\n'\n",
      " '5. From clue 4, the dieter who lost 7 lbs (Stuart) is either the person who '\n",
      " 'used the caveman diet or Raymond. Since the gluten-free dieter lost 7 lbs, '\n",
      " 'the dieter who lost 7 lbs could not have been Raymond, so Stuart used the '\n",
      " 'gluten-free diet.\\n'\n",
      " '6. Since Stuart used the gluten-free diet, Raymond did not, and Wesley did '\n",
      " 'not use the vegan diet, and Stuart did not use the vegan diet, and Raymond '\n",
      " 'did not use the gluten-free diet, and Wesley did not use the gluten-free '\n",
      " 'diet, Tom must have used the vegan diet. Raymond lost 5 lbs and Wesley lost '\n",
      " '5 lbs, but each person lost a different amount, so this cannot be true. '\n",
      " \"Let's re-evaluate\\n\"\n",
      " '1. Stuart lost 7 lbs\\n'\n",
      " '2. Wesley lost 5 lbs\\n'\n",
      " '3. Vegan diet resulted in 3 lb loss\\n'\n",
      " \"4. Gluten-free dieter lost 2 more lbs than Raymond, so Raymond can't have \"\n",
      " \"7lbs. Also, since Raymond isn't vegan, gluten-free, or 7 lbs, Raymond either \"\n",
      " 'lost 9 or 5 lbs. The gluten-free dieter lost 2 more, which would be 11 and 7 '\n",
      " \"respectively. 11 isn't valid, so we need to have Raymond lost 5 and the \"\n",
      " 'gluten-free lost 7\\n'\n",
      " '5. Stuart lost 7 lbs, and the 7 lb dieter is either caveman or Raymond. '\n",
      " 'Since the gluten-free dieter lost 7 lbs, Stuart is the gluten-free. Since '\n",
      " 'Wesley lost 5 lbs, and Raymond lost 5 lbs, this cannot be true.\\n'\n",
      " '\\n'\n",
      " \"Ok, let's try this\\n\"\n",
      " '1. Stuart lost 7\\n'\n",
      " '2. Wesley lost 5\\n'\n",
      " '3. Vegan lost 3\\n'\n",
      " '4. Gluten-free = Raymond + 2\\n'\n",
      " '5. Stuart (7) = caveman or Raymond\\n'\n",
      " '\\n'\n",
      " \"From 4, since Raymond + 2, Raymond can't have 7 (Stuart), 3(vegan) or \"\n",
      " '5(wesley). So Raymond must have lost 9 lbs, and gluten-free lost 11, which '\n",
      " 'is impossible. We need a number lower than 7, so that Raymond + 2 results in '\n",
      " 'something under 10. Vegan = 3, Wesley = 5, Stuart = 7. So Raymond + 2 = '\n",
      " 'someone under 10.\\n'\n",
      " 'This means the Raymond can only have 3 or 5 pounds lost\\n'\n",
      " 'Stuart lost 4 more pounds than the vegan (3lbs), thus Stuart lost 7.\\n'\n",
      " '\\n'\n",
      " 'Stuart is either Raymond or caveman. Raymond cannot be 7, Raymond cannot be '\n",
      " '3, Raymond cannot be 5. Raymond is 9! Gluten free (Raymond + 2 = 11, '\n",
      " 'impossible).\\n'\n",
      " '\\n'\n",
      " \"There's an error in the instructions, as 3 people lost different amounts, \"\n",
      " 'only to have Raymond and Wesley lose the same amount. The clues above '\n",
      " 'conflict with the original premise. Since Wesley lost 5lbs, and Raymond lost '\n",
      " '5lbs, they lost the same amount.\\n'\n",
      " '\\n'\n",
      " 'Final Answer:\\n'\n",
      " '3 | Tom | vegan\\n'\n",
      " '5 | Wesley | caveman\\n'\n",
      " '7 | Stuart | gluten-free\\n'\n",
      " '9 | Raymond | dairy-free\\n')\n"
     ]
    }
   ],
   "source": [
    "pprint(df_gemini[\"gemini-2.0-flash-exp\"].iloc[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
