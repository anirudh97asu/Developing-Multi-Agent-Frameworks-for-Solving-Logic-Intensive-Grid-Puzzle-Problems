{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "from src.llm_engine.gemini_agent import Agent\n",
    "\n",
    "#agent = ChatAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"../data/GridPuzzle_processed.csv\")\n",
    "df = df[df[\"id\"].isin([13, 821, 687, 718, 350, 1084, 1677, 1437, 1597, 1935, 1020, 1495, 2893, 2208, 2700, 2788, 3720, 3936, 3642, 3659, 4465, 4658, 4087, 4861, 4140, 4464, 4848, 3954, 5740, 5675, 5212, 4976, 5889, 5659, 5325, 5585, 6843, 6292, 6045, 6432, 6823, 5923, 6778, 6997, 7685, 7865, 7183, 7389, 7866, 7188, 7837, 8712, 7959, 8356, 8142, 7993, 8519, 9106, 9719, 9139, 9487, 8853, 8988, 9043, 9497, 9596, 9965, 10037, 10380, 10100, 10450, 10440, 9976, 10251, 11592, 11097, 11022, 11603, 11546, 11748, 11263, 11367, 11407, 11304, 11321, 11923, 12703, 12356, 12535, 12172, 12133, 12395, 12818, 13003, 12900, 13407, 13198, 12782, 13051, 12980])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 15)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Clues:\\n'\n",
      " '1. Stuart lost 4 more pounds than the person who used the vegan diet.\\n'\n",
      " '2. The dieter who used the gluten-free diet lost 2 more pounds than '\n",
      " 'Raymond.\\n'\n",
      " '3. Stuart lost 7 lbs.\\n'\n",
      " '4. The dieter who lost 7 lbs is either the person who used the caveman diet '\n",
      " 'or Raymond.\\n'\n",
      " '5. Wesley lost 5 lbs.\\n'\n",
      " 'pounds lost : 3, 5, 7, 9.\\n'\n",
      " 'names : Raymond, Stuart, Tom, Wesley.\\n'\n",
      " 'diets : caveman, dairy-free, gluten-free, vegan.')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(df[\"Clues\"].iloc[0] + \"\\n\"+ df[\"categories\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_name = \"llm_agent\"\n",
    "# system_prompt = \"\"\"You are a helpful AI assistant for logic grid puzzles. When given clues and categories:\n",
    "\n",
    "# Break down each clue into detailed logical steps.\n",
    "# Look for hidden connections between different clues.\n",
    "# Explain your reasoning clearly, one step at a time.\n",
    "# If a clue seems unclear, check your previous steps for helpful information.\n",
    "# Converge after you have processed all the clues\n",
    "\n",
    "# Your goal is to help people understand how to solve the puzzle through logical deduction. Show your thought-process only\n",
    "# without the final answer\"\"\"\n",
    "\n",
    "\n",
    "system_prompt =\"\"\"As a logic grid puzzle assistant, analyze relationships by linking category items to relevant clues:\n",
    "\n",
    "    Item Analysis:\n",
    "\n",
    "        For each item, identify all applicable clues.\n",
    "\n",
    "        Note confirmations, eliminations, or constraints.\n",
    "\n",
    "    Clue Tracking:\n",
    "\n",
    "        Assign clues to items, flagging conflicts or reinforcements.\n",
    "\n",
    "    Conclusion Types:\n",
    "\n",
    "        State definitive connections when clear.\n",
    "\n",
    "        List possibilities when multiple options exist.\n",
    "\n",
    "    Review Process:\n",
    "\n",
    "        Recheck mappings if inconsistencies emerge.\n",
    "\n",
    "    Completion:\n",
    "\n",
    "        Stop when all items are fully assessed.\n",
    "\n",
    "Ensure every item in one category is uniquely mapped to one item from other categories based on categories\n",
    "\n",
    "You should be constrained based on the given clues.\n",
    "\n",
    "Goal: Provide conclusive steps about different item mappings by connecting them with clues\"\"\"\n",
    "\n",
    "agent = Agent(agent_name=agent_name,system_prompt=system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df[\"question\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = int(input())\n",
    "\n",
    "clues = df[\"Clues\"].iloc[ind]\n",
    "categories = df[\"categories\"].iloc[ind]\n",
    "\n",
    "user_query = df.question.iloc[0].split(\"\\n\")[0] + clues + \"\\n\" + \"\\n\\n\"\n",
    "\n",
    "for index, val in enumerate(categories.split(\"\\n\")):\n",
    "    user_query += f\"Category {index +1} \\n\" + val + \"\\n\"\n",
    "\n",
    "\n",
    "llm_agent_response = agent.perform_action(user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clues:\n",
      "1. Toni was born 9 minutes after Peggy.\n",
      "2. The baby born at 12:10am was the Blanchards'.\n",
      "3. Luis wasn't the Vaughans'.\n",
      "4. Iris was born 3 minutes before the Ortegas' baby.\n",
      "5. The Ortegas' child was born 9 minutes before the Quinns' baby.\n",
      "6. Susan was the Blanchards'.\n"
     ]
    }
   ],
   "source": [
    "print(clues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, I understand. I made an error in my previous response. I incorrectly assigned Luis to the Quinn family. I will correct this and provide the accurate solution.\n",
      "\n",
      "**1. Initial Setup:**\n",
      "\n",
      "We have three categories: Times, Names, and Families. Our goal is to match one item from each category to form a complete set.\n",
      "\n",
      "**2. Analyzing the Clues:**\n",
      "\n",
      "*   **Clue 1:** Toni was born 9 minutes after Peggy.\n",
      "*   **Clue 2:** The baby born at 12:10am was the Blanchards'.\n",
      "*   **Clue 3:** Luis wasn't the Vaughans'.\n",
      "*   **Clue 4:** Iris was born 3 minutes before the Ortegas' baby.\n",
      "*   **Clue 5:** The Ortegas' child was born 9 minutes before the Quinns' baby.\n",
      "*   **Clue 6:** Susan was the Blanchards'.\n",
      "\n",
      "**3. Deductions:**\n",
      "\n",
      "*   **From Clue 6:** Susan = Blanchard\n",
      "*   **From Clue 2:** Blanchard = 12:10am\n",
      "*   **Therefore:** Susan = Blanchard = 12:10am\n",
      "\n",
      "*   **From Clue 4 & 5:** Iris -> Ortega -> Quinn (in that order of birth) with 3 minutes between Iris and Ortega, and 9 minutes between Ortega and Quinn.\n",
      "*   **This means:** Iris, Ortega, and Quinn cannot be 12:10am or 12:13am (because there wouldn't be enough time slots after them).\n",
      "\n",
      "*   **From Clue 1:** Peggy and Toni have a 9-minute difference in birth time.\n",
      "*   **This means:** They cannot be 12:01am, 12:04am, or 12:13am (because there wouldn't be enough time slots before or after them).\n",
      "\n",
      "**4. More Deductions:**\n",
      "\n",
      "*   **Combining the deductions about Iris, Ortega, and Quinn:** The only possible times for them are 12:01am, 12:04am, and 12:07am.\n",
      "*   **Therefore:** Quinn = 12:07am, Ortega = 12:04am, Iris = 12:01am\n",
      "\n",
      "*   **Combining the deductions about Peggy and Toni:** The only possible times for them are 12:04am, 12:13am.\n",
      "*   **Since Ortega = 12:04am:** Peggy and Toni must be 12:04am and 12:13am.\n",
      "*   **From Clue 1:** Toni was born 9 minutes after Peggy.\n",
      "*   **Therefore:** Peggy = 12:04am, Toni = 12:13am\n",
      "\n",
      "*   **We now have:**\n",
      "    *   Susan = Blanchard = 12:10am\n",
      "    *   Iris = Ortega = 12:01am\n",
      "    *   Peggy = 12:04am\n",
      "    *   Toni = 12:13am\n",
      "    *   Luis = 12:07am\n",
      "\n",
      "*   **We know:**\n",
      "    *   Susan = Blanchard\n",
      "    *   Iris = Ortega\n",
      "    *   Peggy = Estrada or Quinn or Vaughan\n",
      "    *   Toni = Estrada or Quinn or Vaughan\n",
      "    *   Luis = Estrada or Quinn or Vaughan\n",
      "\n",
      "*   **From Clue 3:** Luis isn't the Vaughans'.\n",
      "*   **Therefore:** Luis = Estrada or Quinn\n",
      "\n",
      "*   **We know:**\n",
      "    *   Susan = Blanchard\n",
      "    *   Iris = Ortega\n",
      "    *   Peggy = Estrada or Quinn or Vaughan\n",
      "    *   Toni = Estrada or Quinn or Vaughan\n",
      "    *   Luis = Estrada or Quinn\n",
      "\n",
      "*   **Since Peggy and Toni are the only ones left, and Luis cannot be Vaughan:**\n",
      "*   **Therefore:** Toni = Vaughan\n",
      "\n",
      "*   **We now have:**\n",
      "    *   Susan = Blanchard = 12:10am\n",
      "    *   Iris = Ortega = 12:01am\n",
      "    *   Peggy = 12:04am\n",
      "    *   Toni = Vaughan = 12:13am\n",
      "    *   Luis = Quinn = 12:07am\n",
      "\n",
      "*   **The only family left is Estrada, and the only name left is Peggy.**\n",
      "*   **Therefore:** Peggy = Estrada\n",
      "\n",
      "**5. Final Solution:**\n",
      "\n",
      "*   Iris = Ortega = 12:01am\n",
      "*   Peggy = Estrada = 12:04am\n",
      "*   Luis = Quinn = 12:07am\n",
      "*   Susan = Blanchard = 12:10am\n",
      "*   Toni = Vaughan = 12:13am\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "print(llm_agent_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A group of friends has decided to try several different weight-loss diets and exercises to see who amongst them can lose the most weight in 3 months. Using only the clues below, match the pounds lost to the options from names and diets. Remember, as with all grid-based logic puzzles, no option in any category will ever be used more than once.\n",
      "\n",
      "pounds lost : 3, 5, 7, 9.\n",
      "names : Raymond, Stuart, Tom, Wesley.\n",
      "diets : caveman, dairy-free, gluten-free, vegan.\n",
      "\n",
      "Clues:\n",
      "1. Stuart lost 4 more pounds than the person who used the vegan diet.\n",
      "2. The dieter who used the gluten-free diet lost 2 more pounds than Raymond.\n",
      "3. Stuart lost 7 lbs.\n",
      "4. The dieter who lost 7 lbs is either the person who used the caveman diet or Raymond.\n",
      "5. Wesley lost 5 lbs.\n",
      "\n",
      "While answering use the following format:\n",
      "Step-by-step solution:\n",
      "Your steps showing how you are solving the puzzle\n",
      "Final Answer:\n",
      "Fill the following table to show your final answer.\n",
      "3 | correct option from names | correct option from diets\n",
      "5 | correct option from names | correct option from diets\n",
      "7 | correct option from names | correct option from diets\n",
      "9 | correct option from names | correct option from diets\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df[\"question\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def initialize_probability_tables(num_categories, categories, cat_topics):\n",
    "    \"\"\"\n",
    "    Initialize uniform probability matrices for all pairs of categories.\n",
    "    \n",
    "    Parameters:\n",
    "    num_categories: int - Number of categories\n",
    "    categories: list - List of category strings with comma-separated subcategories\n",
    "    \n",
    "    Returns:\n",
    "    matrices: dict - Maps category index tuples to probability matrices\n",
    "    category_mapping: dict - Maps topic strings to category index tuples\n",
    "    \n",
    "    Example:\n",
    "    >>> matrices, mapping = initialize_probability_tables(3, [\"red, blue\", \"small, large\", \"cheap, expensive\"])\n",
    "    \"\"\"\n",
    "    r = 2\n",
    "    combinations_ls = list(combinations(np.arange(num_categories), r))\n",
    "    matrices = {}\n",
    "    for index, combination in enumerate(combinations_ls):\n",
    "        c1 , c2 = categories[combination[0]].split(\", \"), categories[combination[1]].split(\", \")\n",
    "       \n",
    "        matrix =  np.full((len(c1), len(c2)), 1 / max(len(c1), len(c2)))\n",
    "        topic = cat_topics[combination[0]] + \"__x__\" + cat_topics[combination[1]]\n",
    "        matrices[topic] = matrix\n",
    "       \n",
    "    return matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> ['pounds lost', 'names', 'diets']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "cat_topics = []\n",
    "categories = []\n",
    "cat_str = df[\"categories\"].iloc[0].split(\"\\n\")\n",
    "for item in cat_str:\n",
    "    cat_topics.append(item.split(\" : \")[0])\n",
    "    categories.append(item.split(\" : \")[1])\n",
    "\n",
    "print(type(cat_topics), cat_topics)\n",
    "\n",
    "probability_matrices = initialize_probability_tables(len(categories), categories, cat_topics) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pounds lost__x__names': array([[0.25, 0.25, 0.25, 0.25],\n",
       "        [0.25, 0.25, 0.25, 0.25],\n",
       "        [0.25, 0.25, 0.25, 0.25],\n",
       "        [0.25, 0.25, 0.25, 0.25]]),\n",
       " 'pounds lost__x__diets': array([[0.25, 0.25, 0.25, 0.25],\n",
       "        [0.25, 0.25, 0.25, 0.25],\n",
       "        [0.25, 0.25, 0.25, 0.25],\n",
       "        [0.25, 0.25, 0.25, 0.25]]),\n",
       " 'names__x__diets': array([[0.25, 0.25, 0.25, 0.25],\n",
       "        [0.25, 0.25, 0.25, 0.25],\n",
       "        [0.25, 0.25, 0.25, 0.25],\n",
       "        [0.25, 0.25, 0.25, 0.25]])}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probability_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "def populate_categories(categories, index):\n",
    "    \n",
    "    cat_topics = []\n",
    "    categories = []\n",
    "\n",
    "    cat_str = df[\"categories\"].iloc[index].split(\"\\n\")\n",
    "    for item in cat_str:\n",
    "        cat_topics.append(item.split(\" : \")[0])\n",
    "        categories.append(item.split(\" : \")[1].split(\", \"))\n",
    "\n",
    "    examples = {}\n",
    "    r = 2\n",
    "    combinations_ls = list(combinations(np.arange(len(cat_topics)), r))\n",
    "\n",
    "    for topic_cob in combinations_ls:\n",
    "        values = []\n",
    "        key_1, key_2 = topic_cob\n",
    "        for i in range(len(categories[0])):\n",
    "            for j in range(len(categories[0])):\n",
    "\n",
    "                values.append((categories[key_1][i], categories[key_2][j]))\n",
    "\n",
    "        examples[cat_topics[topic_cob[0]] + \"__x__\" + cat_topics[topic_cob[1]]] = values\n",
    "\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'times__x__names': [('12:01am', 'Iris'), ('12:01am', 'Luis'), ('12:01am', 'Peggy'), ('12:01am', 'Susan'), ('12:01am', 'Toni.'), ('12:04am', 'Iris'), ('12:04am', 'Luis'), ('12:04am', 'Peggy'), ('12:04am', 'Susan'), ('12:04am', 'Toni.'), ('12:07am', 'Iris'), ('12:07am', 'Luis'), ('12:07am', 'Peggy'), ('12:07am', 'Susan'), ('12:07am', 'Toni.'), ('12:10am', 'Iris'), ('12:10am', 'Luis'), ('12:10am', 'Peggy'), ('12:10am', 'Susan'), ('12:10am', 'Toni.'), ('12:13am.', 'Iris'), ('12:13am.', 'Luis'), ('12:13am.', 'Peggy'), ('12:13am.', 'Susan'), ('12:13am.', 'Toni.')], 'times__x__families': [('12:01am', 'Blanchard'), ('12:01am', 'Estrada'), ('12:01am', 'Ortega'), ('12:01am', 'Quinn'), ('12:01am', 'Vaughan.'), ('12:04am', 'Blanchard'), ('12:04am', 'Estrada'), ('12:04am', 'Ortega'), ('12:04am', 'Quinn'), ('12:04am', 'Vaughan.'), ('12:07am', 'Blanchard'), ('12:07am', 'Estrada'), ('12:07am', 'Ortega'), ('12:07am', 'Quinn'), ('12:07am', 'Vaughan.'), ('12:10am', 'Blanchard'), ('12:10am', 'Estrada'), ('12:10am', 'Ortega'), ('12:10am', 'Quinn'), ('12:10am', 'Vaughan.'), ('12:13am.', 'Blanchard'), ('12:13am.', 'Estrada'), ('12:13am.', 'Ortega'), ('12:13am.', 'Quinn'), ('12:13am.', 'Vaughan.')], 'names__x__families': [('Iris', 'Blanchard'), ('Iris', 'Estrada'), ('Iris', 'Ortega'), ('Iris', 'Quinn'), ('Iris', 'Vaughan.'), ('Luis', 'Blanchard'), ('Luis', 'Estrada'), ('Luis', 'Ortega'), ('Luis', 'Quinn'), ('Luis', 'Vaughan.'), ('Peggy', 'Blanchard'), ('Peggy', 'Estrada'), ('Peggy', 'Ortega'), ('Peggy', 'Quinn'), ('Peggy', 'Vaughan.'), ('Susan', 'Blanchard'), ('Susan', 'Estrada'), ('Susan', 'Ortega'), ('Susan', 'Quinn'), ('Susan', 'Vaughan.'), ('Toni.', 'Blanchard'), ('Toni.', 'Estrada'), ('Toni.', 'Ortega'), ('Toni.', 'Quinn'), ('Toni.', 'Vaughan.')]}\n"
     ]
    }
   ],
   "source": [
    "index=ind\n",
    "examples = populate_categories(categories=categories, index=index)\n",
    "print(examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# system_prompt = f\"\"\"You are a logical reasoning agent.\n",
    "\n",
    "#                     Given a complex sequential list of logical statements about n-categories and corresponding n-2D matrices depicting inter-category\n",
    "#                     item mappings, systematically deconstruct each logical deduction. Critically evaluate the relational insights, translating these\n",
    "#                     abstract logical statements into quantitative probability weights for specific matrix cells. \n",
    "#                     The weighting process should be nuanced, reflecting the contextual strength, coherence, and inferential power of each logical\n",
    "#                     statement as it relates to the underlying categorical relationships.\n",
    "                    \n",
    "#                     Comprehending Deductions & Updating Probabilities\n",
    "                        \n",
    "#                         Carefully process each logical deduction, ensuring clarity by merging sequential steps when appropriate.\n",
    "#                         Use conclusive deductions to:\n",
    "\n",
    "#                             Identify the relevant probability matrix that requires updating.\n",
    "#                             Determine the exact cells to modify, assigning new weights based on the strength of the deduction.\n",
    "\n",
    "#                     Dynamic Adjustments\n",
    "\n",
    "#                         Continuously reassess all decisions as new information becomes available.\n",
    "#                         If new deductions conflict with prior reasoning, promptly revise the probability matrix or discard incorrect conclusions.\n",
    "#                         Ensure that all updates reflect the most accurate and up-to-date analysis.\n",
    "\n",
    "#                     Use these probability values if the information you find fall into one of the categories:\n",
    "                        \n",
    "#                         - Definite: 1.0\n",
    "#                         - Eliminated: 0.0\n",
    "#                         - Uncertain: 0.5\n",
    "#                         - Most Likely: 0.6-0.9\n",
    "                    \n",
    "#                     Remember, as with all grid-based logic puzzles, no option in any category will ever be used more than once.\n",
    "#                     Your task is to maintain precision and adaptability, ensuring logical consistency in every update.\n",
    "#                 \"\"\"\n",
    "\n",
    "# agent_job = f\"\"\"Process the given  resources below and give the output as expected in the format below.\n",
    "                \n",
    "#                 Available Resources:\n",
    "#                         'Logical Step-By-Step Deductions': {llm_agent_response},\n",
    "#                         'Initial Probability Matrices': {probability_matrices},\n",
    "#                         'Category Mapping': {df[\"categories\"].iloc[3]}\n",
    "\n",
    "#                 - You should use the format below\n",
    "\n",
    "#                     *** (\"matrix_name\", probability_value, row_index, column_index)\n",
    "\n",
    "#                 Return the sequence of probability updates\n",
    "#             \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f\"\"\" You are a probabilistic agent who assess scenarios and logical deductions based on which assign \n",
    "                     weightages between 0.05 - 0.95. Your roles include:\n",
    "                     \n",
    "                     For every item-item pair given to you, you should assess if they fall under definite, important, \n",
    "                     uncertain (any item might have multiple pairs) or useless.\n",
    "                     \n",
    "                     Assign a weight for each item-item pair based on the above range.\n",
    "\n",
    "                     Repeat until all item-item pairs are processed.\n",
    "\n",
    "                     Return the final answer.\n",
    "\n",
    "                \"\"\"\n",
    "agent_job = f\"\"\"Process the given  resources below and give the output as expected in the format below.\n",
    "                \n",
    "                Available Resources:\n",
    "                        'Logical Step-By-Step Deductions': {llm_agent_response},\n",
    "                        'Item-Item Pair Dictionary': {examples},\n",
    "                    \n",
    "            \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'times : 12:01am, 12:04am, 12:07am, 12:10am, 12:13am.\\nnames : Iris, Luis, Peggy, Susan, Toni.\\nfamilies : Blanchard, Estrada, Ortega, Quinn, Vaughan.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'ingredients': [{'Item_Item_Pair': \"('12:01am', 'Iris')\", 'Probability': 0.95}, {'Item_Item_Pair': \"('12:01am', 'Luis')\", 'Probability': 0.05}, {'Item_Item_Pair': \"('12:01am', 'Peggy')\", 'Probability': 0.05}, {'Item_Item_Pair': \"('12:01am', 'Susan')\", 'Probability': 0.05}, {'Item_Item_Pair': \"('12:01am', 'Toni.')\", 'Probability': 0.05}, {'Item_Item_Pair': \"('12:04am', 'Iris')\", 'Probability': 0.05}, {'Item_Item_Pair': \"('12:04am', 'Luis')\", 'Probability': 0.05}, {'Item_Item_Pair': \"('12:04am', 'Peggy')\", 'Probability': 0.95}, {'Item_Item_Pair': \"('12:04am', 'Susan')\", 'Probability': 0.05}, {'Item_Item_Pair': \"('12:04am', 'Toni.')\", 'Probability': 0.05}, {'Item_Item_Pair': \"('12:07am', 'Iris')\", 'Probability': 0.05}, {'Item_Item_Pair': \"('12:07am', 'Luis')\", 'Probability': 0.95}, {'Item_Item_Pair': \"('12:07am', 'Peggy')\", 'Probability': 0.05}, {'Item_Item_Pair': \"('12:07am', 'Susan')\", 'Probability': 0.05}, {'Item_Item_Pair': \"('12:07am', 'Toni.')\", 'Probability': 0.05}, {'Item_Item_Pair': \"('12:10am', 'Iris')\", 'Probability': 0.05}, {'Item_Item_Pair': \"('12:10am', 'Luis')\", 'Probability': 0.05}, {'Item_Item_Pair': \"('12:10am', 'Peggy')\", 'Probability': 0.05}, {'Item_Item_Pair': \"('12:10am', 'Susan')\", 'Probability': 0.95}, {'Item_Item_Pair': \"('12:10am', 'Toni.')\", 'Probability': 0.05}, {'Item_Item_Pair': \"('12:13am.', 'Iris')\", 'Probability': 0.05}, {'Item_Item_Pair': \"('12:13am.', 'Luis')\", 'Probability': 0.05}, {'Item_Item_Pair': \"('12:13am.', 'Peggy')\", 'Probability': 0.05}, {'Item_Item_Pair': \"('12:13am.', 'Susan')\", 'Probability': 0.05}, {'Item_Item_Pair': \"('12:13am.', 'Toni.')\", 'Probability': 0.95}, {'Item_Item_Pair': \"('12:01am', 'Blanchard')\", 'Probability': 0.05}, {'Item_Item_Pair': \"('12:01am', 'Estrada')\", 'Probability': 0.05}, {'Item_Item_Pair': \"('12:01am', 'Ortega')\", 'Probability': 0.95}, {'Item_Item_Pair': \"('12:01am', 'Quinn')\", 'Probability': 0.05}, {'Item_Item_Pair': \"('12:01am', 'Vaughan.')\", 'Probability': 0.05}, {'Item_Item_Pair': \"('12:04am', 'Blanchard')\", 'Probability': 0.05}, {'Item_Item_Pair': \"('12:04am', 'Estrada')\", 'Probability': 0.95}, {'Item_Item_Pair': \"('12:04am', 'Ortega')\", 'Probability': 0.05}, {'Item_Item_Pair': \"('12:04am', 'Quinn')\", 'Probability': 0.05}, {'Item_Item_Pair': \"('12:04am', 'Vaughan.')\", 'Probability': 0.05}, {'Item_Item_Pair': \"('12:07am', 'Blanchard')\", 'Probability': 0.05}, {'Item_Item_Pair': \"('12:07am', 'Estrada')\", 'Probability': 0.05}, {'Item_Item_Pair': \"('12:07am', 'Ortega')\", 'Probability': 0.05}, {'Item_Item_Pair': \"('12:07am', 'Quinn')\", 'Probability': 0.95}, {'Item_Item_Pair': \"('12:07am', 'Vaughan.')\", 'Probability': 0.05}, {'Item_Item_Pair': \"('12:10am', 'Blanchard')\", 'Probability': 0.95}, {'Item_Item_Pair': \"('12:10am', 'Estrada')\", 'Probability': 0.05}, {'Item_Item_Pair': \"('12:10am', 'Ortega')\", 'Probability': 0.05}, {'Item_Item_Pair': \"('12:10am', 'Quinn')\", 'Probability': 0.05}, {'Item_Item_Pair': \"('12:10am', 'Vaughan.')\", 'Probability': 0.05}, {'Item_Item_Pair': \"('12:13am.', 'Blanchard')\", 'Probability': 0.05}, {'Item_Item_Pair': \"('12:13am.', 'Estrada')\", 'Probability': 0.05}, {'Item_Item_Pair': \"('12:13am.', 'Ortega')\", 'Probability': 0.05}, {'Item_Item_Pair': \"('12:13am.', 'Quinn')\", 'Probability': 0.05}, {'Item_Item_Pair': \"('12:13am.', 'Vaughan.')\", 'Probability': 0.95}, {'Item_Item_Pair': \"('Iris', 'Blanchard')\", 'Probability': 0.05}, {'Item_Item_Pair': \"('Iris', 'Estrada')\", 'Probability': 0.05}, {'Item_Item_Pair': \"('Iris', 'Ortega')\", 'Probability': 0.95}, {'Item_Item_Pair': \"('Iris', 'Quinn')\", 'Probability': 0.05}, {'Item_Item_Pair': \"('Iris', 'Vaughan.')\", 'Probability': 0.05}, {'Item_Item_Pair': \"('Luis', 'Blanchard')\", 'Probability': 0.05}, {'Item_Item_Pair': \"('Luis', 'Estrada')\", 'Probability': 0.05}, {'Item_Item_Pair': \"('Luis', 'Ortega')\", 'Probability': 0.05}, {'Item_Item_Pair': \"('Luis', 'Quinn')\", 'Probability': 0.95}, {'Item_Item_Pair': \"('Luis', 'Vaughan.')\", 'Probability': 0.05}, {'Item_Item_Pair': \"('Peggy', 'Blanchard')\", 'Probability': 0.05}, {'Item_Item_Pair': \"('Peggy', 'Estrada')\", 'Probability': 0.95}, {'Item_Item_Pair': \"('Peggy', 'Ortega')\", 'Probability': 0.05}, {'Item_Item_Pair': \"('Peggy', 'Quinn')\", 'Probability': 0.05}, {'Item_Item_Pair': \"('Peggy', 'Vaughan.')\", 'Probability': 0.05}, {'Item_Item_Pair': \"('Susan', 'Blanchard')\", 'Probability': 0.95}, {'Item_Item_Pair': \"('Susan', 'Estrada')\", 'Probability': 0.05}, {'Item_Item_Pair': \"('Susan', 'Ortega')\", 'Probability': 0.05}, {'Item_Item_Pair': \"('Susan', 'Quinn')\", 'Probability': 0.05}, {'Item_Item_Pair': \"('Susan', 'Vaughan.')\", 'Probability': 0.05}, {'Item_Item_Pair': \"('Toni.', 'Blanchard')\", 'Probability': 0.05}, {'Item_Item_Pair': \"('Toni.', 'Estrada')\", 'Probability': 0.05}, {'Item_Item_Pair': \"('Toni.', 'Ortega')\", 'Probability': 0.05}, {'Item_Item_Pair': \"('Toni.', 'Quinn')\", 'Probability': 0.05}, {'Item_Item_Pair': \"('Toni.', 'Vaughan.')\", 'Probability': 0.95}]}]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "prob_agent = agent = Agent(agent_name=\"prob_agent_1\", system_prompt=system_prompt, json=True)\n",
    "\n",
    "\n",
    "prob_agent_resp = prob_agent.perform_action(agent_job)\n",
    "\n",
    "print(json.loads(prob_agent_resp))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "system_prompt = f\"\"\"You are an expert in probability theory and you are given a bunch of item-item pairs and their corresponding\n",
    "                    probabilities. Conclude the final values of category mappings by forming a grid. It should be in the below format \n",
    "\n",
    "                    Final Answer:\\n'\n",
    "                    'Fill your answers as a table.\\n'\n",
    "                                    \n",
    "                \"\"\"\n",
    "\n",
    "agent_job = f\"\"\"Analyze the probability matrix and look for the final solution\n",
    "                The responses are: {prob_agent_resp},\n",
    "                The categories are: {categories}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      " Final Answer:\n",
      " \n",
      "\n",
      " '\n",
      " | Time    | Name   | Family    |\n",
      " | :------ | :----- | :-------- |\n",
      " | 12:01am | Iris   | Ortega    |\n",
      " | 12:04am | Peggy  | Estrada   |\n",
      " | 12:07am | Luis   | Quinn     |\n",
      " | 12:10am | Susan  | Blanchard |\n",
      " | 12:13am | Toni.  | Vaughan.  |\n",
      " '\n",
      " ```\n"
     ]
    }
   ],
   "source": [
    "solution_agent_system_prompt = system_prompt\n",
    "\n",
    "solution_agent = Agent(agent_name=\"solution_agent\", system_prompt=solution_agent_system_prompt)\n",
    "\n",
    "\n",
    "solution_agent_resp = solution_agent.perform_action(agent_job)\n",
    "\n",
    "print(solution_agent_resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('12:01am | Iris | Vaughan\\n'\n",
      " '12:04am | Peggy | Ortega\\n'\n",
      " '12:07am | Luis | Estrada\\n'\n",
      " '12:10am | Susan | Blanchard\\n'\n",
      " '12:13am | Toni | Quinn')\n"
     ]
    }
   ],
   "source": [
    "pprint(df[\"answer\"].iloc[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from z3 import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raymond: 3 lbs, diet=3\n",
      "Stuart: 7 lbs, diet=0\n",
      "Tom: 9 lbs, diet=1\n",
      "Wesley: 5 lbs, diet=-1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Variables\n",
    "Raymond, Stuart, Tom, Wesley = Ints('Raymond Stuart Tom Wesley')\n",
    "diet_R, diet_S, diet_T, diet_W = Ints('diet_R diet_S diet_T diet_W')\n",
    "\n",
    "# Diet encodings\n",
    "caveman, dairy_free, gluten_free, vegan = 0, 1, 2, 3\n",
    "\n",
    "s = Solver()\n",
    "\n",
    "# Possible pounds and diets\n",
    "s.add(Or(Raymond == 3, Raymond == 5, Raymond == 7, Raymond == 9))\n",
    "s.add(Or(Stuart == 3, Stuart == 5, Stuart == 7, Stuart == 9))\n",
    "s.add(Or(Tom == 3, Tom == 5, Tom == 7, Tom == 9))\n",
    "s.add(Or(Wesley == 3, Wesley == 5, Wesley == 7, Wesley == 9))\n",
    "\n",
    "s.add(Distinct(Raymond, Stuart, Tom, Wesley))\n",
    "s.add(Distinct(diet_R, diet_S, diet_T, diet_W))\n",
    "\n",
    "# Clue 3: Stuart lost 7 lbs\n",
    "s.add(Stuart == 7)\n",
    "\n",
    "# Clue 5: Wesley lost 5 lbs\n",
    "s.add(Wesley == 5)\n",
    "\n",
    "# Clue 1: Stuart lost 4 more than vegan dieter\n",
    "s.add(Stuart == 4 + If(diet_R == vegan, Raymond,\n",
    "                     If(diet_S == vegan, Stuart,\n",
    "                     If(diet_T == vegan, Tom, Wesley))))\n",
    "\n",
    "# Clue 2: Gluten-free dieter lost 2 more than Raymond\n",
    "s.add(If(diet_R == gluten_free, Raymond == Raymond + 2,\n",
    "       If(diet_S == gluten_free, Stuart == Raymond + 2,\n",
    "       If(diet_T == gluten_free, Tom == Raymond + 2,\n",
    "          Wesley == Raymond + 2))))\n",
    "\n",
    "# Clue 4: 7-lb loser is caveman or Raymond\n",
    "s.add(Or(diet_S == caveman, Raymond == 7))\n",
    "\n",
    "# Solve\n",
    "if s.check() == sat:\n",
    "    m = s.model()\n",
    "    print(f\"Raymond: {m[Raymond]} lbs, diet={m[diet_R]}\")\n",
    "    print(f\"Stuart: {m[Stuart]} lbs, diet={m[diet_S]}\")\n",
    "    print(f\"Tom: {m[Tom]} lbs, diet={m[diet_T]}\")\n",
    "    print(f\"Wesley: {m[Wesley]} lbs, diet={m[diet_W]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(r\"../data/GridPuzzle_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6    3 | Raymond | vegan\\n5 | Wesley | gluten-free\\...\n",
      "Name: answer, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "res = df[df[\"id\"] == 13][\"answer\"]\n",
    "pprint(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
